{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1af212db",
   "metadata": {},
   "source": [
    "# Hidden Markov Models with DMX-Learn\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides a comprehensive walkthrough of **Hidden Markov Models (HMMs)** using the `dmx-learn` library. You'll learn what HMMs are, why they're useful, and how to implement them effectively.\n",
    "\n",
    "## What are Hidden Markov Models?\n",
    "\n",
    "Hidden Markov Models are powerful probabilistic models for sequential data where:\n",
    "- The system being modeled is assumed to be a **Markov process** with hidden (unobserved) states\n",
    "- We observe **emissions** that depend on these hidden states\n",
    "- The goal is to infer the hidden state sequence and learn the model parameters from observed data\n",
    "\n",
    "## Why Use HMMs?\n",
    "\n",
    "HMMs are particularly useful for:\n",
    "- **Time series analysis**: Modeling sequential data with temporal dependencies\n",
    "- **Speech recognition**: Identifying phonemes and words from audio signals\n",
    "- **Bioinformatics**: Gene prediction, protein sequence analysis, and genome annotation\n",
    "- **Natural language processing**: Part-of-speech tagging and named entity recognition\n",
    "- **Financial modeling**: Regime detection in market conditions\n",
    "- **Activity recognition**: Inferring hidden states from sensor data\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "In this notebook, we'll cover:\n",
    "1. **Basic HMM concepts**: States, transitions, and emissions\n",
    "2. **Creating HMMs in dmx-learn**: Setting up model structure and parameters\n",
    "3. **Training HMMs**: Learning model parameters from data using Expectation-Maximization (EM)\n",
    "4. **Inference**: Decoding hidden state sequences and computing likelihoods\n",
    "5. **Estimation tricks**: Key'ing distributions and flattening parameter estimates.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c00934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "import re\n",
    "import codecs\n",
    "from dmx.stats import *\n",
    "from dmx.utils.estimation import optimize, best_of\n",
    "from dmx.utils.optsutil import count_by_value\n",
    "DATA_LOC = \"../data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a107271",
   "metadata": {},
   "source": [
    "## Understanding Hidden Markov Models: Mathematical Framework\n",
    "\n",
    "### Model Components\n",
    "\n",
    "A Hidden Markov Model consists of:\n",
    "\n",
    "1. **Hidden States** $S = \\{s_1, s_2, ..., s_N\\}$: The unobserved states of the system\n",
    "2. **Observations** $O = \\{o_1, o_2, ..., o_M\\}$: The observable outputs/emissions\n",
    "3. **Initial State Distribution** $\\pi$: Probability of starting in each hidden state\n",
    "   - $\\pi_i = P(z_1 = s_i)$\n",
    "4. **Transition Probabilities** $A$: Probability of moving from one hidden state to another\n",
    "   - $a_{ij} = P(z_{t+1} = s_j \\mid z_t = s_i)$\n",
    "5. **Emission Probabilities** $B$: Probability of observing output given hidden state\n",
    "   - $b_j(o_k) = P(x_t = o_k \\mid z_t = s_j)$\n",
    "6. **Length Distribution**: Probability of observing output sequence of a given length $T$\n",
    "   - $P(T=t)$\n",
    "\n",
    "### The Likelihood Function\n",
    "\n",
    "Given a sequence of observations $X = (x_1, x_2, ..., x_T)$ and a hidden state sequence $Z = (z_1, z_2, ..., z_T)$, the joint probability is:\n",
    "\n",
    "$$P(X, Z \\mid \\theta) = P(z_1) \\prod_{t=1}^{T-1} P(z_{t+1} \\mid z_t) \\prod_{t=1}^{T} P(x_t \\mid z_t) P(T)$$\n",
    "\n",
    "Where $\\theta = \\{\\pi, A, B\\}$ represents all model parameters.\n",
    "\n",
    "The likelihood of observing $X$ (marginalizing over all possible hidden state sequences) is:\n",
    "\n",
    "$$P(X \\mid \\theta) = \\sum_{Z} P(X, Z \\mid \\theta) = \\sum_{Z} \\pi_{z_1} \\prod_{t=1}^{T-1} a_{z_t, z_{t+1}} \\prod_{t=1}^{T} b_{z_t}(x_t)P(T)$$\n",
    "\n",
    "### The `HiddenMarkovModelDistribution` in `dmx-learn`\n",
    "Below we will call the `help()` function on `HiddenMarkovModelDistribution` to see what parameters are required to define an HMM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb77c3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class HiddenMarkovModelDistribution in module dmx.stats.hidden_markov:\n",
      "\n",
      "class HiddenMarkovModelDistribution(dmx.stats.pdist.SequenceEncodableProbabilityDistribution)\n",
      " |  HiddenMarkovModelDistribution(\n",
      " |      topics: Sequence[dmx.stats.pdist.SequenceEncodableProbabilityDistribution],\n",
      " |      w: Union[Sequence[float], numpy.ndarray],\n",
      " |      transitions: Union[List[List[float]], numpy.ndarray],\n",
      " |      taus: Union[List[List[float]], numpy.ndarray, NoneType] = None,\n",
      " |      len_dist: Optional[dmx.stats.pdist.SequenceEncodableProbabilityDistribution] = NullDistribution(name=None),\n",
      " |      name: Optional[str] = None,\n",
      " |      keys: Optional[Tuple[Optional[str], Optional[str], Optional[str]]] = (None, None, None),\n",
      " |      terminal_values: Optional[Set[~T]] = None,\n",
      " |      use_numba: bool = False\n",
      " |  ) -> None\n",
      " |\n",
      " |  HiddenMarkovModelDistribution object defining HMM compatible with data type T.\n",
      " |\n",
      " |  Defines an HMM with emission distributions in 'topics' (all must have the same data type T). If a length\n",
      " |  distribution for the length of HMM sequence is included, it must have data type int with support of non-negative\n",
      " |  integers.\n",
      " |\n",
      " |  Attributes:\n",
      " |      topics (Sequence[SequenceEncodableProbabilityDistribution]): Emission distributions all having type T.\n",
      " |      n_topics (int): Number of emission distributions.\n",
      " |      n_states (int): Number of hidden states.\n",
      " |      w (np.ndarray): Initial state probabilities.\n",
      " |      log_w (np.ndarray): Initial state log-probabilities.\n",
      " |      transitions (np.ndarray): 2-d Numpy array of hidden state transition probabilities. (n_states by n_states).\n",
      " |      log_transitions (np.ndarray): Log of above.\n",
      " |      taus (Optional[np.ndarray]): Emission distributions are a Mixture over topics. Hidden states govern\n",
      " |          transitions between mixture weights.\n",
      " |      log_taus (Optional[np.ndarray]): Log probabilties of taus above.\n",
      " |      has_topics (bool): True if taus is passed.\n",
      " |      len_dist (Optional[SequenceEncodableProbabilityDistribution]):\n",
      " |      name (Optional[str]): Set name to object instance.\n",
      " |      terminal_values (Optional[Set[T]]): Define terminating emission outputs of the HMM.\n",
      " |      use_numba (bool): If True, use numba package for encoding and vectorized operations.\n",
      " |      keys (Tuple[Optional[str], Optional[str], Optional[str]]): Keys for initial states, transitions counts, and\n",
      " |          emission distributions. Defaults to Tuple of (None, None, None).\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      HiddenMarkovModelDistribution\n",
      " |      dmx.stats.pdist.SequenceEncodableProbabilityDistribution\n",
      " |      dmx.stats.pdist.ProbabilityDistribution\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(\n",
      " |      self,\n",
      " |      topics: Sequence[dmx.stats.pdist.SequenceEncodableProbabilityDistribution],\n",
      " |      w: Union[Sequence[float], numpy.ndarray],\n",
      " |      transitions: Union[List[List[float]], numpy.ndarray],\n",
      " |      taus: Union[List[List[float]], numpy.ndarray, NoneType] = None,\n",
      " |      len_dist: Optional[dmx.stats.pdist.SequenceEncodableProbabilityDistribution] = NullDistribution(name=None),\n",
      " |      name: Optional[str] = None,\n",
      " |      keys: Optional[Tuple[Optional[str], Optional[str], Optional[str]]] = (None, None, None),\n",
      " |      terminal_values: Optional[Set[~T]] = None,\n",
      " |      use_numba: bool = False\n",
      " |  ) -> None\n",
      " |      HiddenMarkovModelDistribution object.\n",
      " |\n",
      " |      Args:\n",
      " |          topics (Sequence[SequenceEncodableProbabilityDistribution]): Emission distributions all having type T.\n",
      " |          w (Union[Sequence[float], np.ndarray]): Initial state probabilities.\n",
      " |          transitions (Union[List[List[float]], np.ndarray]): 2-d array of hidden state transition probabilities.\n",
      " |          taus (Optional[Union[Sequence[float], np.ndarray]]): Emission distributions are a Mixture over topics.\n",
      " |              Hidden states govern transitions between mixture weights.\n",
      " |          len_dist (Optional[SequenceEncodableProbabilityDistribution]):\n",
      " |          name (Optional[str]): Set name to object instance.\n",
      " |          keys (Tuple[Optional[str], Optional[str], Optional[str]]): Keys for initial states, transitions counts, and\n",
      " |              emission distributions. Defaults to Tuple of (None, None, None).\n",
      " |          terminal_values (Optional[Set[T]]): Define terminating emission outputs of the HMM.\n",
      " |          use_numba (bool): If True, use numba package for encoding and vectorized operations.\n",
      " |\n",
      " |  __str__(self) -> str\n",
      " |      Return str(self).\n",
      " |\n",
      " |  density(self, x: Sequence[~T]) -> float\n",
      " |      Returns the density of HMM for an observed sequence x.\n",
      " |\n",
      " |      See 'HiddenMarkovDistribution.log_density()' for details.\n",
      " |\n",
      " |      Args:\n",
      " |          x (Sequence[T]): Observed sequence of HMM emissions.\n",
      " |\n",
      " |      Returns:\n",
      " |          float: Density of HMM for observed sequence x.\n",
      " |\n",
      " |  dist_to_encoder(self) -> 'HiddenMarkovDataEncoder'\n",
      " |      Create DataSequenceEncoder object for SequenceEncodableProbabilityDistribution instance.\n",
      " |\n",
      " |      Returns:\n",
      " |          DataSequenceEncoder\n",
      " |\n",
      " |  estimator(self, pseudo_count: Optional[float] = None) -> 'HiddenMarkovEstimator'\n",
      " |      Create a ParameterEstimator for corresponding SequenceEncodableProbabilityDistribution.\n",
      " |\n",
      " |      Args:\n",
      " |          pseudo_count (Optional[float]): Regularize sufficient statistics in estimation step.\n",
      " |\n",
      " |      Returns:\n",
      " |          ParameterEstimator\n",
      " |\n",
      " |  log_density(self, x: Sequence[~T]) -> float\n",
      " |      Returns the log-density of HMM for observed sequence x.\n",
      " |\n",
      " |      Density for a sequence of length N is given by recursively evaluating the conditional density,\n",
      " |\n",
      " |          p_mat(x_mat(0),x_mat(1),....,x_mat(t)) = p_mat(x_mat(t)|x_mat(0),...,x_mat(t-1)) = p_mat(x_mat(t)|Z(t))*p_mat(Z(t)|Z(t-1))*p_mat(Z(t-1)|x_mat(0),....,x_mat(t-1))\n",
      " |\n",
      " |      for t = 1,2,...,N-1. p_mat(Z(0)) is given by 'w', p_mat(x_mat(t)|Z(t)) is given by emission distribution 'topics' for\n",
      " |      t = 0,1,...,N-1.\n",
      " |\n",
      " |      The returned density is given by\n",
      " |\n",
      " |          p_mat(x_mat) = p_mat(x_mat(0),x_mat(1),....,x_mat(t))*P_len(N).\n",
      " |\n",
      " |      where P_len(N) is the length distribution 'len_dist', if assigned.\n",
      " |      Note: All calculations are done on the log scale with log-sum-exp used to prevent numerical underflow.\n",
      " |\n",
      " |      If 'has_topics' is true, 'weighed_log_sum_exp' and 'log_sum' calls from dmx.utils.vector are used to handle\n",
      " |      the emission distributions being treated as mixture distributions with weights 'log_taus'.\n",
      " |\n",
      " |      Args:\n",
      " |          x (Sequence[T]): Observed sequence of HMM emissions.\n",
      " |\n",
      " |      Returns:\n",
      " |          float: Log-density of observed HMM sequence x.\n",
      " |\n",
      " |  sampler(self, seed: Optional[int] = None) -> 'HiddenMarkovSampler'\n",
      " |      Create a DistributionSampler object for a given ProbabilityDistribution.\n",
      " |\n",
      " |      Args:\n",
      " |          seed (Optional[int]): Set seed for drawing samples from distribution.\n",
      " |\n",
      " |  seq_log_density(self, x: 'HiddenMarkovEncodedDataSequence') -> 'np.ndarray'\n",
      " |      Vectorized evaluation of the log density.\n",
      " |\n",
      " |      Args:\n",
      " |          x (EncodedDataSequence): EncodedDataSequence for corresponding SequenceEncodedProbabilityDistribution.\n",
      " |\n",
      " |      Returns:\n",
      " |          np.ndarray\n",
      " |\n",
      " |  seq_posterior(self, x: 'HiddenMarkovEncodedDataSequence') -> List[numpy.ndarray]\n",
      " |      Compute posterior distribution for each latent state of a sequence.\n",
      " |\n",
      " |      Args:\n",
      " |          x (HiddenMarkovEncodedDataSequence): Numba encoded sequence of HMM observations.\n",
      " |\n",
      " |      Returns:\n",
      " |          List[np.ndarray]: A list of posterior probabilities for each latent state for each observation sequence.\n",
      " |\n",
      " |  seq_viterbi(self, x: 'HiddenMarkovEncodedDataSequence') -> numpy.ndarray\n",
      " |      Vectorized Viterbi sequence for sequence of HMM observations.\n",
      " |\n",
      " |      Notes:\n",
      " |          This takes a numba encoded sequence of HMM observations and returns back the flattened 1-d sequence of\n",
      " |          Viterbi states.\n",
      " |\n",
      " |      Args:\n",
      " |          x (HiddenMarkovEncodedDataSequence): Numba EncodedDataSequence for Hidden Markov Model.\n",
      " |\n",
      " |  viterbi(self, x: Sequence[~T]) -> numpy.ndarray\n",
      " |      Returns the viterbi sequence for an HMM observation.\n",
      " |\n",
      " |      Args:\n",
      " |          x (Sequence[T]): Single HMM sequence.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from dmx.stats.pdist.SequenceEncodableProbabilityDistribution:\n",
      " |\n",
      " |  seq_ld_lambda(self)\n",
      " |\n",
      " |  seq_log_density_lambda(self)\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from dmx.stats.pdist.ProbabilityDistribution:\n",
      " |\n",
      " |  __eq__(self, other: Any) -> bool\n",
      " |      Tests if a ProbabilityDistribution is equivilent to another.\n",
      " |\n",
      " |      Args:\n",
      " |          other (Any): Object to test against.\n",
      " |\n",
      " |      Returns:\n",
      " |          True if the objects match.\n",
      " |\n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from dmx.stats.pdist.ProbabilityDistribution:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from dmx.stats.pdist.ProbabilityDistribution:\n",
      " |\n",
      " |  __hash__ = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(HiddenMarkovModelDistribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6a9b97",
   "metadata": {},
   "source": [
    "### Mapping Mathematical Parameters to `HiddenMarkovModelDistribution`\n",
    "\n",
    "Now let's understand how the mathematical components of an HMM map to the parameters in `dmx-learn`:\n",
    "\n",
    "| Mathematical Component | Parameter Name | Description | Example |\n",
    "|------------------------|----------------|-------------|---------|\n",
    "| **Initial Distribution** $\\pi$ | `w` | Probability distribution over initial hidden states | `[0.6, 0.4]` for sunny/rainy |\n",
    "| **Transition Probabilities** $A$ | `transitions` | Distribution modeling state-to-state transitions | Markov chain or transition matrix |\n",
    "| **Emission Probabilities** $B$ | `topics` | Distributions from `dmx.stats` that model observations given hidden states | List of `CategoricalDistribution` objects |\n",
    "| **Length Distribution** | `len_dist` | Distribution from `dmx.stats` to model the length of obeserved sequences | `PossionDistribution(lam=5.0)`|\n",
    "\n",
    "**Key Insights:**\n",
    "\n",
    "- **`w` (weights)**: Represents $\\pi$, the initial state distribution\n",
    "  - Vector of probabilities, one for each hidden state\n",
    "  - Must sum to 1.0\n",
    "  \n",
    "- **`transitions`**: Represents $A$, the transition model\n",
    "  - Rows sum to 1.0\n",
    "  - Defines how the hidden state evolves over time\n",
    "  - Each row represents transition probabilities from one state\n",
    "  \n",
    "- **`topics`**: Represents $B$, the emission/observation model\n",
    "  - List of probability distributions (one per hidden state)\n",
    "  - Each distribution models what observations are likely given that hidden state\n",
    "  - Can be any distribution from `dmx.stats` (Categorical, Gaussian, etc.)\n",
    "\n",
    "- **`len_dist`**: Models the length of the sequences.\n",
    "    - Ignored if not passed. \n",
    "    - Must be passed to generate (sample) sequences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adad67f",
   "metadata": {},
   "source": [
    "### Toy Example: Weather and Activities\n",
    "\n",
    "Let's consider a simple example to make this concrete:\n",
    "\n",
    "**Scenario**: You're observing someone's daily activities for a week at a time, and you want to infer the weather (which you cannot directly observe).\n",
    "\n",
    "- **Hidden States**: Weather conditions\n",
    "  - $S = \\{\\text{Sunny}, \\text{Rainy}\\}$\n",
    "  \n",
    "- **Observations**: Activities performed\n",
    "  - $O = \\{\\text{walk}, \\text{shop}, \\text{clean}\\}$\n",
    "\n",
    "**Model Parameters**:\n",
    "\n",
    "*Initial Distribution* $\\pi$:\n",
    "- $P(z_1 = \\text{Sunny}) = 0.6$\n",
    "- $P(z_1 = \\text{Rainy}) = 0.4$\n",
    "\n",
    "*Transition Probabilities* $A$:\n",
    "$$A = \\begin{bmatrix} \n",
    "P(\\text{Sunny} \\mid \\text{Sunny}) & P(\\text{Rainy} \\mid \\text{Sunny}) \\\\\n",
    "P(\\text{Sunny} \\mid \\text{Rainy}) & P(\\text{Rainy} \\mid \\text{Rainy})\n",
    "\\end{bmatrix} = \\begin{bmatrix} \n",
    "0.7 & 0.3 \\\\\n",
    "0.4 & 0.6\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "*Emission Probabilities* $B$:\n",
    "$$B = \\begin{bmatrix}\n",
    "P(\\text{walk} \\mid \\text{Sunny}) & P(\\text{shop} \\mid \\text{Sunny}) & P(\\text{clean} \\mid \\text{Sunny}) \\\\\n",
    "P(\\text{walk} \\mid \\text{Rainy}) & P(\\text{shop} \\mid \\text{Rainy}) & P(\\text{clean} \\mid \\text{Rainy})\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0.6 & 0.3 & 0.1 \\\\\n",
    "0.1 & 0.4 & 0.5\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "**Interpretation**:\n",
    "- On sunny days, people are more likely to walk (0.6) than clean (0.1)\n",
    "- On rainy days, people are more likely to clean (0.5) than walk (0.1)\n",
    "- Sunny weather tends to persist (0.7 probability of staying sunny)\n",
    "- Rainy weather is somewhat persistent (0.6 probability of staying rainy)\n",
    "\n",
    "**Example Sequence**:\n",
    "If we observe the sequence: $X = (\\text{walk}, \\text{shop}, \\text{clean})$\n",
    "\n",
    "We can compute the likelihood for any hidden state sequence. For example, if $Z = (\\text{Sunny}, \\text{Sunny}, \\text{Rainy})$:\n",
    "\n",
    "$$P(X, Z) = P(\\text{Sunny}) \\times P(\\text{walk} \\mid \\text{Sunny}) \\times P(\\text{Sunny} \\mid \\text{Sunny}) \\times P(\\text{shop} \\mid \\text{Sunny}) \\times P(\\text{Rainy} \\mid \\text{Sunny}) \\times P(\\text{clean} \\mid \\text{Rainy})$$\n",
    "\n",
    "$$= 0.6 \\times 0.6 \\times 0.7 \\times 0.3 \\times 0.3 \\times 0.5 = 0.01134$$\n",
    "\n",
    "The total likelihood $P(X)$ would sum this over all $2^3 = 8$ possible hidden state sequences.\n",
    "\n",
    "### Defining the `HiddenMarkovModelDistribution`\n",
    "We can define the `HiddenMarkovModelDistribution` in `dmx-learn` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a5fec41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial states for sunny and rainy\n",
    "w = np.array([0.6, 0.4])\n",
    "# define the transition matrix\n",
    "tpm = [[0.7, 0.3], [0.4, 0.6]]\n",
    "\n",
    "# define the topic distributions \n",
    "d1 = CategoricalDistribution(pmap={\"walk\": 0.6, \"shop\": 0.3, \"clean\": 0.1})\n",
    "d2 = CategoricalDistribution(pmap={\"walk\": 0.1, \"shop\": 0.4, \"clean\": 0.5})\n",
    "\n",
    "# assume that we always observe 7 days in the week\n",
    "len_dist = CategoricalDistribution(pmap={7: 1.0})\n",
    "d = HiddenMarkovModelDistribution(topics=[d1, d2], transitions=tpm, w=w, len_dist=len_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a052d2",
   "metadata": {},
   "source": [
    "We generate `1000` sequences using the `sampler()` call on the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "892b28ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: ['clean', 'walk', 'walk', 'shop', 'shop', 'clean', 'walk']\n",
      "Sample 1: ['shop', 'shop', 'clean', 'walk', 'walk', 'shop', 'walk']\n",
      "Sample 2: ['walk', 'walk', 'shop', 'shop', 'shop', 'clean', 'walk']\n",
      "Sample 3: ['clean', 'shop', 'clean', 'walk', 'clean', 'walk', 'walk']\n",
      "Sample 4: ['walk', 'walk', 'walk', 'walk', 'clean', 'clean', 'walk']\n",
      "Sample 5: ['walk', 'shop', 'walk', 'clean', 'clean', 'clean', 'clean']\n",
      "Sample 6: ['clean', 'shop', 'clean', 'shop', 'clean', 'clean', 'shop']\n",
      "Sample 7: ['walk', 'walk', 'clean', 'shop', 'clean', 'clean', 'walk']\n",
      "Sample 8: ['shop', 'walk', 'walk', 'walk', 'walk', 'shop', 'shop']\n",
      "Sample 9: ['shop', 'clean', 'clean', 'clean', 'shop', 'walk', 'clean']\n"
     ]
    }
   ],
   "source": [
    "data = d.sampler(seed=1).sample(1000)\n",
    "print('\\n'.join(['Sample %d: '%(i) + str(data[i]) for i in range(10)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faeb116d",
   "metadata": {},
   "source": [
    "## The Viterbi Algorithm: Finding the Most Likely Hidden State Sequence\n",
    "\n",
    "### What is the Viterbi Algorithm?\n",
    "\n",
    "The **Viterbi algorithm** is a dynamic programming algorithm that finds the most probable sequence of hidden states given a sequence of observations. Unlike computing the full likelihood (which sums over all possible hidden state sequences), Viterbi finds the single best path.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "Given observations $X = (x_1, x_2, ..., x_T)$, we want to find:\n",
    "\n",
    "$$Z^* = \\arg\\max_{Z} P(Z \\mid X) = \\arg\\max_{Z} P(X, Z)$$\n",
    "\n",
    "The algorithm uses dynamic programming with the recursion:\n",
    "\n",
    "$$\\delta_t(j) = \\max_{i} [\\delta_{t-1}(i) \\cdot a_{ij}] \\cdot b_j(x_t)$$\n",
    "\n",
    "where:\n",
    "- $\\delta_t(j)$ is the maximum probability of being in state $j$ at time $t$ given the observations up to time $t$\n",
    "- $a_{ij}$ is the transition probability from state $i$ to state $j$\n",
    "- $b_j(x_t)$ is the emission probability of observing $x_t$ in state $j$\n",
    "\n",
    "**Initialization:**\n",
    "$$\\delta_1(j) = \\pi_j \\cdot b_j(x_1)$$\n",
    "\n",
    "**Termination:**\n",
    "$$P(Z^*) = \\max_j \\delta_T(j)$$\n",
    "\n",
    "### Why Use Viterbi?\n",
    "\n",
    "- **Decoding**: Infer the most likely hidden state sequence (e.g., what was the weather each day?)\n",
    "- **Efficient**: $O(N^2T)$ complexity where $N$ is number of states and $T$ is sequence length\n",
    "- **Optimal**: Guaranteed to find the globally optimal state sequence\n",
    "- **Applications**: Speech recognition, bioinformatics (gene finding), activity recognition\n",
    "\n",
    "### Viterbi in the Weather/Activities Example\n",
    "\n",
    "Let's apply Viterbi to our toy example. Suppose we observe:\n",
    "$$X = (\\text{walk}, \\text{shop}, \\text{clean})$$\n",
    "\n",
    "The Viterbi algorithm will determine: \"What's the most likely weather pattern that would produce these activities?\". We can easily run the Viterbi algorithm with a call to `viterbi()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ecc68ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: ['clean', 'walk', 'walk', 'shop', 'shop', 'clean', 'walk']\n",
      "Viterbi: ['sunny', 'rainy', 'rainy', 'rainy', 'rainy', 'sunny', 'rainy']\n",
      "\n",
      "Sample: ['shop', 'shop', 'clean', 'walk', 'walk', 'shop', 'walk']\n",
      "Viterbi: ['rainy', 'sunny', 'sunny', 'rainy', 'rainy', 'rainy', 'rainy']\n",
      "\n",
      "Sample: ['walk', 'walk', 'shop', 'shop', 'shop', 'clean', 'walk']\n",
      "Viterbi: ['rainy', 'rainy', 'rainy', 'rainy', 'rainy', 'sunny', 'rainy']\n",
      "\n",
      "Sample: ['clean', 'shop', 'clean', 'walk', 'clean', 'walk', 'walk']\n",
      "Viterbi: ['sunny', 'sunny', 'sunny', 'rainy', 'sunny', 'rainy', 'rainy']\n",
      "\n",
      "Sample: ['walk', 'walk', 'walk', 'walk', 'clean', 'clean', 'walk']\n",
      "Viterbi: ['rainy', 'rainy', 'rainy', 'rainy', 'sunny', 'sunny', 'rainy']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "int_to_hidden_state = {0: 'rainy', 1: 'sunny'}\n",
    "v_int = [d.viterbi(u) for u in data[:5]]\n",
    "v_str = [[int_to_hidden_state[w] for w in v] for v in v_int]\n",
    "\n",
    "print('\\n'.join(['Sample: ' + str(data[i]) + '\\nViterbi: ' + str(v_str[i]) + '\\n' for i in range(len(v_str))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daf757b",
   "metadata": {},
   "source": [
    "## Parameter Estimation: Learning HMMs from Data\n",
    "\n",
    "### The Challenge of Parameter Estimation\n",
    "\n",
    "In real-world applications, we typically don't know the true HMM parameters ($\\pi$, $A$, $B$). Instead, we have:\n",
    "- **Observed data**: Sequences of observations (e.g., daily activities)\n",
    "- **Unknown parameters**: Hidden states, transition probabilities, and emission probabilities\n",
    "\n",
    "**Goal**: Learn the model parameters that best explain the observed data.\n",
    "\n",
    "First we are going to define the `HiddenMarkovEstimator`. \n",
    "\n",
    "\n",
    "**Key Parameters:**\n",
    "- **`estimators`**: A list of estimators for the topics.\n",
    "- **`len_estimator`**: A length estimator for the length distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ede0f4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "est0 = CategoricalEstimator()\n",
    "est = HiddenMarkovEstimator(\n",
    "    estimators=[est0]*2, \n",
    "    len_estimator=est0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10082c8c",
   "metadata": {},
   "source": [
    "We can use `optimize` to fit the HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2764f60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100: ln[p_mat(Data|Model)]=-7.601640e+03, ln[p_mat(Data|Model)]-ln[p_mat(Data|PrevModel)]=1.001307e-01\n",
      "Iteration 200: ln[p_mat(Data|Model)]=-7.571739e+03, ln[p_mat(Data|Model)]-ln[p_mat(Data|PrevModel)]=3.750003e-03\n",
      "Iteration 300: ln[p_mat(Data|Model)]=-7.571414e+03, ln[p_mat(Data|Model)]-ln[p_mat(Data|PrevModel)]=2.833165e-03\n",
      "Iteration 400: ln[p_mat(Data|Model)]=-7.571164e+03, ln[p_mat(Data|Model)]-ln[p_mat(Data|PrevModel)]=2.199573e-03\n",
      "Iteration 300: ln[p_mat(Data|Model)]=-7.571414e+03, ln[p_mat(Data|Model)]-ln[p_mat(Data|PrevModel)]=2.833165e-03\n",
      "Iteration 400: ln[p_mat(Data|Model)]=-7.571164e+03, ln[p_mat(Data|Model)]-ln[p_mat(Data|PrevModel)]=2.199573e-03\n",
      "Iteration 500: ln[p_mat(Data|Model)]=-7.570971e+03, ln[p_mat(Data|Model)]-ln[p_mat(Data|PrevModel)]=1.684310e-03\n",
      "Iteration 600: ln[p_mat(Data|Model)]=-7.570825e+03, ln[p_mat(Data|Model)]-ln[p_mat(Data|PrevModel)]=1.259340e-03\n",
      "Iteration 500: ln[p_mat(Data|Model)]=-7.570971e+03, ln[p_mat(Data|Model)]-ln[p_mat(Data|PrevModel)]=1.684310e-03\n",
      "Iteration 600: ln[p_mat(Data|Model)]=-7.570825e+03, ln[p_mat(Data|Model)]-ln[p_mat(Data|PrevModel)]=1.259340e-03\n",
      "Iteration 700: ln[p_mat(Data|Model)]=-7.570716e+03, ln[p_mat(Data|Model)]-ln[p_mat(Data|PrevModel)]=9.221931e-04\n",
      "Iteration 800: ln[p_mat(Data|Model)]=-7.570638e+03, ln[p_mat(Data|Model)]-ln[p_mat(Data|PrevModel)]=6.667273e-04\n",
      "Iteration 700: ln[p_mat(Data|Model)]=-7.570716e+03, ln[p_mat(Data|Model)]-ln[p_mat(Data|PrevModel)]=9.221931e-04\n",
      "Iteration 800: ln[p_mat(Data|Model)]=-7.570638e+03, ln[p_mat(Data|Model)]-ln[p_mat(Data|PrevModel)]=6.667273e-04\n",
      "Iteration 900: ln[p_mat(Data|Model)]=-7.570581e+03, ln[p_mat(Data|Model)]-ln[p_mat(Data|PrevModel)]=4.801958e-04\n",
      "Iteration 1000: ln[p_mat(Data|Model)]=-7.570540e+03, ln[p_mat(Data|Model)]-ln[p_mat(Data|PrevModel)]=3.472387e-04\n",
      "Iteration 900: ln[p_mat(Data|Model)]=-7.570581e+03, ln[p_mat(Data|Model)]-ln[p_mat(Data|PrevModel)]=4.801958e-04\n",
      "Iteration 1000: ln[p_mat(Data|Model)]=-7.570540e+03, ln[p_mat(Data|Model)]-ln[p_mat(Data|PrevModel)]=3.472387e-04\n"
     ]
    }
   ],
   "source": [
    "fit = optimize(\n",
    "    data=data,\n",
    "    estimator=est,\n",
    "    rng=np.random.RandomState(10),\n",
    "    init_p=0.10,\n",
    "    max_its=1000,\n",
    "    delta=1.0e-4,\n",
    "    print_iter=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1add06",
   "metadata": {},
   "source": [
    "Lets inspect the estimated initial states and transition matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce3a2268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition Probability Matrix: \n",
      " array([[0.59770099, 0.40229901],\n",
      "       [0.19625044, 0.80374956]]):\n",
      "\n",
      "Initial Weights: \n",
      " array([0.28064907, 0.71935093]):\n"
     ]
    }
   ],
   "source": [
    "print('Transition Probability Matrix: \\n %s:'%(repr(fit.transitions)))\n",
    "print('\\nInitial Weights: \\n %s:'%(repr(fit.w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6581f10",
   "metadata": {},
   "source": [
    "These look reasonable. Now we can examine the estimated topics (emission distributions). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2756b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: CategoricalDistribution({'clean': 0.5293729757856719, 'shop': 0.40647816522832164, 'walk': 0.06414885898600642}, default_value=0.0, name=None, keys=None)\n",
      "\n",
      "Topic 1: CategoricalDistribution({'clean': 0.1457312028273226, 'shop': 0.3083313690267165, 'walk': 0.5459374281459609}, default_value=0.0, name=None, keys=None)\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\n'.join(['Topic %i: '%(i) + repr(fit.topics[i]) for i in range(2)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6d7381",
   "metadata": {},
   "source": [
    "All the sequences had length 7, so our length distribution should reflect that..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e9c3a1a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CategoricalDistribution({7: 1.0}, default_value=0.0, name=None, keys=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit.len_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f586794",
   "metadata": {},
   "source": [
    "Obviously there is not going to be an alignment with meaning and the hidden states. Earlier we were able to examine the weather as hidden states since we knew what they were before hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988291e3",
   "metadata": {},
   "source": [
    "## Generalizing the HMM\n",
    "The toy example we started with was a simple two state HMM with text emissions. This is not very exciting and can likely be handled with user written code or a different package. \n",
    "\n",
    "Lets look at an example now where `dmx-learn` can really help out. As per the norm, we will consider heterogenous tuples of data. Lets consider a case where we jointly observe sequences of `tuple[float, str]`. We will also assume the sequences have varying sizes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0219e85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the topics for a 3 state HMM. \n",
    "v = 0.1\n",
    "d1 = CompositeDistribution(\n",
    "    (GaussianDistribution(-20.0, 1.0), CategoricalDistribution({'a': 0.5 - v, 'b': 0.5 - v, 'c': v, 'd': v})))\n",
    "d2 = CompositeDistribution(\n",
    "    (GaussianDistribution(0.0, 1.0), CategoricalDistribution({'a': v, 'b': v, 'c': 0.5 - v, 'd': 0.5 - v})))\n",
    "d3 = CompositeDistribution(\n",
    "    (GaussianDistribution(20.0, 1.0), CategoricalDistribution({'a': v, 'b': 0.5 - v, 'c': 0.5 - v, 'd': v})))\n",
    "topics = [d1, d2, d3]\n",
    "\n",
    "# define the length distribution\n",
    "len_dist = PoissonDistribution(lam=4.0)\n",
    "\n",
    "# initial state distribution\n",
    "w = [0.4, 0.4, 0.2]\n",
    "\n",
    "# transition distribution\n",
    "tpm = [[0.8, 0.1, 0.1], [0.1, 0.8, 0.1], [0.1, 0.1, 0.8]]\n",
    "\n",
    "# define the Hidden Markov model\n",
    "dist = HiddenMarkovModelDistribution(topics=topics, w=w, transitions=tpm, len_dist=len_dist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e2a2d1",
   "metadata": {},
   "source": [
    "Draw `1000` samples from the HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c761a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = dist.sampler(seed=32)\n",
    "data = sampler.sample(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18d4583",
   "metadata": {},
   "source": [
    "We can print out a few samples.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9bdd963e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: [(20.500, b)]\n",
      "Sample 1: [(18.335, c), (-19.435, b), (-18.976, b), (-19.862, a)]\n",
      "Sample 2: [(-20.280, a), (-0.318, c), (0.145, d), (0.572, d), (-20.958, a)]\n",
      "Sample 3: []\n",
      "Sample 4: [(-18.712, d), (-20.612, b), (-20.871, b), (-21.165, a)]\n"
     ]
    }
   ],
   "source": [
    "for i, x in enumerate(data[:5]):\n",
    "    print('Sample %i: [' %(i) +', '.join(['(%0.3f, %s)'%(xx[0], xx[1]) for xx in x])+']')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8356b5d",
   "metadata": {},
   "source": [
    "You may have noticed that some of the samples are empty. `dmx-learn` can handle this case. It will simply be treated as a sequence of length 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0eb83b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the estimator for the tuple of data\n",
    "est0 = GaussianEstimator()\n",
    "est1 = CategoricalEstimator()\n",
    "est2 = CompositeEstimator([est0, est1])\n",
    "\n",
    "# define the length estimator\n",
    "len_est = PoissonEstimator() \n",
    "\n",
    "# define the estimator for the topics (3 state hmm)\n",
    "topic_ests = [est2] * 3 \n",
    "est = HiddenMarkovEstimator(estimators=topic_ests, len_estimator=est1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9582e2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21: ln[p_mat(Data|Model)]=-1.567520e+04, ln[p_mat(Data|Model)]-ln[p_mat(Data|PrevModel)]=0.000000e+00\n"
     ]
    }
   ],
   "source": [
    "fit = optimize(\n",
    "    data=data,\n",
    "    estimator=est,\n",
    "    rng=np.random.RandomState(45),\n",
    "    init_p=0.10,\n",
    "    max_its=1000,\n",
    "    delta=1.0e-4,\n",
    "    print_iter=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81e87ce",
   "metadata": {},
   "source": [
    "Lets look at the estimated topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "55206f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: CompositeDistribution(dists=[GaussianDistribution(19.981996424401455, 1.0244243730846847, name=None, keys=None),CategoricalDistribution({'a': 0.10598626104023552, 'b': 0.38763493621197254, 'c': 0.4023552502453386, 'd': 0.10402355250245339}, default_value=0.0, name=None, keys=None)], name=None, keys=None)\n",
      "1: CompositeDistribution(dists=[GaussianDistribution(0.010639974870002197, 1.0455403103304144, name=None, keys=None),CategoricalDistribution({'a': 0.1051939513477975, 'b': 0.09072978303747535, 'c': 0.4089414858645628, 'd': 0.39513477975016437}, default_value=0.0, name=None, keys=None)], name=None, keys=None)\n",
      "2: CompositeDistribution(dists=[GaussianDistribution(-19.998408273214554, 0.9110252706064443, name=None, keys=None),CategoricalDistribution({'a': 0.39422431161853594, 'b': 0.3955674949630625, 'c': 0.10476830087306917, 'd': 0.10543989254533244}, default_value=0.0, name=None, keys=None)], name=None, keys=None)\n"
     ]
    }
   ],
   "source": [
    "for i, comp in enumerate(fit.topics):\n",
    "    print(\": \".join([str(i), str(comp)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bdc468",
   "metadata": {},
   "source": [
    "We can also look at the estimated transition matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c5c1623b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True TPM:\n",
      "array([[0.8, 0.1, 0.1],\n",
      "       [0.1, 0.8, 0.1],\n",
      "       [0.1, 0.1, 0.8]])\n",
      "Estimated TPM:\n",
      "array([[0.79483696, 0.10869565, 0.09646739],\n",
      "       [0.11073826, 0.77768456, 0.11157718],\n",
      "       [0.09236234, 0.10657194, 0.80106572]])\n"
     ]
    }
   ],
   "source": [
    "print('True TPM:\\n' + repr(dist.transitions) + '\\nEstimated TPM:\\n' + repr(fit.transitions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ce9cd9",
   "metadata": {},
   "source": [
    "The initial weight estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "316503b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True initial state distribution:\n",
      "array([0.4, 0.4, 0.2])\n",
      "Estimated initial state distribution:\n",
      "array([0.20307692, 0.40410256, 0.39282051])\n"
     ]
    }
   ],
   "source": [
    "print('True initial state distribution:\\n' + repr(dist.w) + '\\nEstimated initial state distribution:\\n' + repr(fit.w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d110d4",
   "metadata": {},
   "source": [
    "Note that in the above, the estimates are correct, however the ordering of the component and weights may be permuted. This happens in estimation, just be sure to do proper book keeping when needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636d7e3b",
   "metadata": {},
   "source": [
    "# Language Identification With Coupled HMMs\n",
    "\n",
    "Now we have worked through a toy example and generalized to see how `dmx-learn` can model sequential dependencies of heterogenous data tuples. We also saw that `dmx-learn` can easily handle sequences of varying lengths.\n",
    "\n",
    "We'll now turn our attention to a data example in which we build a set of models for idenfiying languages with HMMs. We will use text exerpts from the Iliad to train three HMMs on English, French, and Spanish words. What will make this a bit more interesting is that the HMMs will have the same emission distributions. This means that the differences between the languages must be encoded in the transition probabilities and initial state probabilities. \n",
    "\n",
    "We randomly pick spots in each text and extract a fixed length sequence of words, $\\{w_i\\}_{i=1}^m$. The language of the text is appended and we then estimate the distribution \n",
    "$$P(\\{w_i\\}_{i=1}^m | \\text{language}) = \\prod_{i = 1}^m P(w_i | \\text{language}).$$\n",
    "\n",
    "where our model over the sequences of words are an HMM with the same emission distributions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a88d5a4",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "Load each text, switch tolower case, remove the liscense stuff, and split on non-word characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301e6482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import codecs\n",
    "import os\n",
    "\n",
    "fin = codecs.open(os.path.join(DATA_LOC, 'iliad', 'iliad_en.txt'), encoding='utf-8', mode='r')\n",
    "temp_en = fin.read()\n",
    "fin.close()\n",
    "\n",
    "fin = codecs.open(os.path.join(DATA_LOC, 'iliad', 'iliad_fr.txt'), encoding='utf-8', mode='r')\n",
    "temp_fr = fin.read()\n",
    "fin.close()\n",
    "\n",
    "fin = codecs.open(os.path.join(DATA_LOC, 'iliad', 'iliad_es.txt'), encoding='utf-8', mode='r')\n",
    "temp_es = fin.read()\n",
    "fin.close()\n",
    "\n",
    "end_en = 'end of the project gutenberg ebook'\n",
    "end_fr = 'end of the project gutenberg ebook'\n",
    "end_es = 'end of the project gutenberg ebook'\n",
    "\n",
    "start_en = 'sing, goddess, the wrath of achilles'\n",
    "start_fr = 'chante, déesse, du pèlèiade akhilleus'\n",
    "start_es = 'canta, oh diosa, la cólera del pelida aquiles'\n",
    "\n",
    "temp_en = temp_en.lower().split(end_en)[0]\n",
    "temp_fr = temp_fr.lower().split(end_fr)[0]\n",
    "temp_es = temp_es.lower().split(end_es)[0]\n",
    "\n",
    "temp_en = start_en + temp_en.split(start_en, 1)[1]\n",
    "temp_fr = start_fr + temp_fr.split(start_fr, 1)[1]\n",
    "temp_es = start_es + temp_es.split(start_es, 1)[1]\n",
    "\n",
    "# Use raw string for regex\n",
    "temp_en = list(filter(lambda u: len(u) > 0, re.split(r'\\W+', temp_en)))\n",
    "temp_fr = list(filter(lambda u: len(u) > 0, re.split(r'\\W+', temp_fr)))\n",
    "temp_es = list(filter(lambda u: len(u) > 0, re.split(r'\\W+', temp_es)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8edc880",
   "metadata": {},
   "source": [
    "For each language we will randomly pick spots throughout the text and extract text sequences. The true langauge is appended to make the data look like `(language ID, {word, word, ...})`. These are then split into a training (90% of data) and testing set (10% of data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2a7b304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "seq_len = 5\n",
    "num_seq = 1000\n",
    "split_idx = int(num_seq*0.9)\n",
    "\n",
    "idx_en = rng.choice(int(len(temp_en)/seq_len), num_seq, replace=False)\n",
    "idx_fr = rng.choice(int(len(temp_fr)/seq_len), num_seq, replace=False)\n",
    "idx_es = rng.choice(int(len(temp_es)/seq_len), num_seq, replace=False)\n",
    "\n",
    "data_en = [(0, temp_en[(i*seq_len):((i+1)*seq_len)]) for i in idx_en]\n",
    "data_fr = [(1, temp_fr[(i*seq_len):((i+1)*seq_len)]) for i in idx_fr]\n",
    "data_es = [(2, temp_es[(i*seq_len):((i+1)*seq_len)]) for i in idx_es]\n",
    "\n",
    "train_data = data_en[:split_idx] + data_fr[:split_idx] + data_es[:split_idx]\n",
    "test_data  = data_en[split_idx:] + data_fr[split_idx:] + data_es[split_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9776d501",
   "metadata": {},
   "source": [
    "Here are some samples from each language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a1b2a8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, ['pour', 'their', 'wine', 'then', 'would'])\n",
      "(0, ['he', 'saw', 'lying', 'on', 'the'])\n",
      "(0, ['strife', 'and', 'wars', 'and', 'battles'])\n",
      "(0, ['not', 'even', 'one', 'from', 'among'])\n",
      "(0, ['thy', 'dart', 'at', 'this', 'fellow'])\n",
      "(1, ['ont', 'gardé', 'ce', 'qu', 'il'])\n",
      "(1, ['mains', 'vigoureuses', 'un', 'créneau', 'du'])\n",
      "(1, ['même', 'char', 'vit', 'au', 'loin'])\n",
      "(1, ['par', 'serment', 'aux', 'troiens', 'de'])\n",
      "(1, ['dans', 'l', 'olympos', 'qui', 'est'])\n",
      "(2, ['construído', 'trípode', 'pero', 'ni', 'ulises'])\n",
      "(2, ['dentro', 'ya', 'del', 'magnífico', 'palacio'])\n",
      "(2, ['xxii', '46', 'á', '48', '2'])\n",
      "(2, ['obstina', 'en', 'rechazarlas', 'se', 'dirigen'])\n",
      "(2, ['y', 'dijo', 'el', 'eximio', 'vate'])\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(map(str,data_en[0:5])))\n",
    "print('\\n'.join(map(str,data_fr[0:5])))\n",
    "print('\\n'.join(map(str,data_es[0:5])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f279a305",
   "metadata": {},
   "source": [
    "#### Estimation\n",
    "\n",
    "We are going to build an initial estimator and an estimator for fitting the model. First we will need to consider our modeling objective. The model we will estimate is a conditional distribution with three given integer values 0 (English), 1 (Greek), 2 (Spanish) for the languages. The text sequences will be modeled as a sequence of 20 state HMMs with categorical emissions that are shared among each language we have conditioned on. \n",
    "\n",
    "We want to build a model for three languages, where each model is an HMM with the same topics. Our data looks like `tuple[int, list[str]]`. We will use a `ConditionalDistributionEstimator` to handle the different language types, `SequenceEstimator` to handle the sequence of words, and an `HiddenMarkovEstimator` words. Since the emissions of the HMM are strings (letters of the words), we will use a `CategoricalEstimator` for the emission distributions. \n",
    "\n",
    "To ensure that our conditional models share topics for the HMMs, we will need to key the `HiddenMarkovEstimator`. We will also need to flatten our topic distributions to ensure that each letter has positive probability to avoid numerical underflow. These two constraints are met by passing `keys` to the `HiddenMarkovEstimator` and setting a `suff_stat` for the topics. These variables are described by:\n",
    "\n",
    "1. `keys = (None, None, 'topics')`: This will be used to ensure that the emission distributions of the HMM are the same.\n",
    "\n",
    "2. `suff_stat`: This is a dictionary mapping the words of all languages to a positive float. This is merely a method to ensure that each emission distribution has a non-zero probability for each word in the training set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e3650353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will ensure sure that every letter in the training set has non-zero probability\n",
    "all_langs = list(sorted(list(set([u[0] for u in train_data]))))\n",
    "suff_stat = count_by_value([w for u in train_data for v in u[1] for w in v])\n",
    "tcnt = sum(suff_stat.values())\n",
    "suff_stat = {k:v/tcnt for k,v in suff_stat.items()}\n",
    "\n",
    "num_states = 20\n",
    "# define the flattened initial estimator for words\n",
    "iest0 = CategoricalEstimator(pseudo_count=1.0, suff_stat=suff_stat)\n",
    "\n",
    "# define the HMM and initial estimator\n",
    "iest1 = HiddenMarkovEstimator(estimators=[iest0] * num_states, pseudo_count=(1.0, 1.0), keys=(None, None, \"topics\"), use_numba=True)\n",
    "iest2 = SequenceEstimator(iest1)\n",
    "iest = ConditionalDistributionEstimator(estimator_map={u: iest2 for u in all_langs})\n",
    "\n",
    "# define the flattened estimator for words\n",
    "est0 = CategoricalEstimator(pseudo_count=1.0e-6, suff_stat=suff_stat)\n",
    "\n",
    "# define the estimator\n",
    "est1 = HiddenMarkovEstimator(estimators=[est0] * num_states, pseudo_count=(1.0e-6, 1.0e-6), keys=(None, None, \"topics\"), use_numba=True)\n",
    "est2 = SequenceEstimator(est1)\n",
    "est = ConditionalDistributionEstimator(estimator_map={u: est2 for u in all_langs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fb4acda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 250: ln[p_mat(Data|Model)]=-1.401184e+05, ln[p_mat(Data|Model)]-ln[p_mat(Data|PrevModel)]=3.596256e+00\n",
      "Iteration 500: ln[p_mat(Data|Model)]=-1.398345e+05, ln[p_mat(Data|Model)]-ln[p_mat(Data|PrevModel)]=1.362659e+00\n",
      "Iteration 500: ln[p_mat(Data|Model)]=-1.398345e+05, ln[p_mat(Data|Model)]-ln[p_mat(Data|PrevModel)]=1.362659e+00\n",
      "Iteration 750: ln[p_mat(Data|Model)]=-1.395108e+05, ln[p_mat(Data|Model)]-ln[p_mat(Data|PrevModel)]=4.127860e-01\n",
      "Iteration 750: ln[p_mat(Data|Model)]=-1.395108e+05, ln[p_mat(Data|Model)]-ln[p_mat(Data|PrevModel)]=4.127860e-01\n",
      "Iteration 1000: ln[p_mat(Data|Model)]=-1.394234e+05, ln[p_mat(Data|Model)]-ln[p_mat(Data|PrevModel)]=5.880658e-02\n",
      "Iteration 1000: ln[p_mat(Data|Model)]=-1.394234e+05, ln[p_mat(Data|Model)]-ln[p_mat(Data|PrevModel)]=5.880658e-02\n"
     ]
    }
   ],
   "source": [
    "fit = optimize(\n",
    "    data=train_data,\n",
    "    estimator=est,\n",
    "    rng=np.random.RandomState(53),\n",
    "    init_p=1.0,\n",
    "    init_estimator=iest,\n",
    "    max_its=1000,\n",
    "    delta=1.0e-4,\n",
    "    print_iter=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf42e3ae",
   "metadata": {},
   "source": [
    "The function `get_classification_matrix` will compute the likelihood of each sample given each language and then select the langauge with the highest likelhood. A matrix of counts, where the row is the true class and the column is the predict class, is returned. The function `pp` will make a fixed precision string for given matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b1d9229d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_matrix(model, data):\n",
    "    \n",
    "    # Extract the labels from the model\n",
    "    cvals = list(sorted(list(model.dmap.keys())))\n",
    "    cvals_cnt = len(cvals)\n",
    "    cvals_map = dict(zip(cvals, range(cvals_cnt)))\n",
    "    \n",
    "    res = np.zeros((cvals_cnt, cvals_cnt))\n",
    "    ll_mat = np.zeros((cvals_cnt, len(data)))\n",
    "    \n",
    "    # Make a list of the true labels\n",
    "    true_idx_list = [cvals_map[u[0]] for u in data]\n",
    "    \n",
    "    for i in range(cvals_cnt):\n",
    "        # Replace the true label with the test label\n",
    "        ll_mat[i,:] = model.seq_log_density(model.dist_to_encoder().seq_encode([(cvals[i], u[1]) for u in data]))\n",
    "        \n",
    "    # Find the most likely language\n",
    "    test_idx_list = np.argmax(ll_mat, axis=0)\n",
    "    \n",
    "    # Enumerate the entries and count the true and predicted languages\n",
    "    for true_idx, test_idx in zip(true_idx_list, test_idx_list):\n",
    "        res[true_idx, test_idx] += 1\n",
    "\n",
    "    return res\n",
    "\n",
    "def pp(x):\n",
    "    return '\\n'.join([','.join(['%0.3f'%(v) for v in x[i,:]]) for i in range(x.shape[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba0e6b3",
   "metadata": {},
   "source": [
    "The results should be pretty good (somewhere around the high 90s) when the classifier is applied to the training set and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7b542f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "0.984,0.004,0.011\n",
      "0.003,0.972,0.024\n",
      "0.002,0.016,0.982\n",
      "Test\n",
      "0.960,0.020,0.020\n",
      "0.000,0.950,0.050\n",
      "0.010,0.030,0.960\n"
     ]
    }
   ],
   "source": [
    "train_res = get_classification_matrix(fit,train_data)\n",
    "print('Train')\n",
    "print(pp(train_res / np.sum(train_res, axis=1, keepdims=True)))\n",
    "\n",
    "test_res = get_classification_matrix(fit,test_data)\n",
    "print('Test')\n",
    "print(pp(test_res / np.sum(test_res, axis=1, keepdims=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fe2beb",
   "metadata": {},
   "source": [
    "So there you have it, we can learn the differences in the languages quite well simply through the transition densities and intial state vectors. \n",
    "\n",
    "This notebook tutorial should serve as a primer on HMMs in `dmx-learn`. We also saw how flattening and key'ing distributions can come in handy for defining models with shared parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DMX 1.0.0 (dmx_kernel)",
   "language": "python",
   "name": "dmx_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
