{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54fe0265",
   "metadata": {},
   "source": [
    "# Outline: User-Defined dmx-learn Class\n",
    "\n",
    "Below is an outline of the order in which we will write each of the following classes for our user-defined model. This notebook will work through each step and slowly build up a full `dmx-learn` distribution for use. \n",
    "\n",
    "## SequenceEncodableProbabilityDistribution\n",
    "\n",
    "1. **Log-likelihood**: First we work out the log-likelihood for a single observation.\n",
    "2. **Vectorize the log-likelihood**: Next we think about how we can format our data for fast vectorized updates.\n",
    "3. **Create DataEncodedSequence**: Stores encoded data.\n",
    "4. **Write the DataSequenceEncoder**: This processes our data for use with vectorized calls.\n",
    "5. **Write a Sampler**: This allows us to draw samples from the distribution.\n",
    "\n",
    "## SequenceEncodableStatisticAccumulator\n",
    "\n",
    "1. **Determine Sufficient Statistics**: Follows from exponential family, defines the member variables of the object.\n",
    "2. **Write Key Functionality**: Allows us to share parameters with other distribution instances.\n",
    "3. **Update**: Define method for sufficient statistic accumulation.\n",
    "4. **Initialize**: Define initialization for the sufficient statistics.\n",
    "5. **Write a factory object**: Standard factory object for the accumulator.\n",
    "\n",
    "## ParameterEstimator\n",
    "\n",
    "1. **Define the wrapper**: This is what users most commonly interact with to estimate distributions.\n",
    "2. **Form estimates**: Use sufficient statistics gathered in the accumulator to estimate the distribution.\n",
    "\n",
    "Once we have filled out our univariate Gaussian distribution, we can compare it with the standard dmx-learn style mixture wrapper. Each of the abstract classes above can be found in the file `dmx.stats.pdist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8f91cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import os\n",
    "\n",
    "import numpy as np \n",
    "from numpy.random import RandomState\n",
    "from typing import Optional, Sequence, Dict, Union, Tuple, Any\n",
    "from dmx.arithmetic import * \n",
    "from dmx.stats.pdist import (SequenceEncodableProbabilityDistribution, \n",
    "                                ParameterEstimator, \n",
    "                                DistributionSampler, \n",
    "                                StatisticAccumulatorFactory, \n",
    "                                SequenceEncodableStatisticAccumulator, \n",
    "                                DataSequenceEncoder, \n",
    "                                EncodedDataSequence)\n",
    "from dmx.utils.estimation import optimize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac21d42d",
   "metadata": {},
   "source": [
    "# One-Dimensional Gaussian Mixture Model (GMM)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "A One-Dimensional Gaussian Mixture Model (GMM) is a probabilistic model that assumes that the data is generated from a mixture of several one-dimensional Gaussian distributions with unknown parameters. This model is useful for clustering and density estimation in one-dimensional data.\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "A one-dimensional GMM is defined as follows:\n",
    "\n",
    "$$p(x) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x | \\mu_k, \\sigma_k^2)$$\n",
    "\n",
    "where:\n",
    "- **K** is the number of Gaussian components.\n",
    "- **π_k** is the weight of the k-th Gaussian component, satisfying $\\sum_{k=1}^{K} \\pi_k = 1$.\n",
    "- $\\mathcal{N}(x | \\mu_k, \\sigma_k^2)$ is the one-dimensional Gaussian distribution with mean **μ_k** and variance **σ_k²**.\n",
    "\n",
    "The one-dimensional Gaussian distribution is given by:\n",
    "\n",
    "$$\\mathcal{N}(x | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "where:\n",
    "- **σ²** is the variance of the Gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816c0466",
   "metadata": {},
   "source": [
    "# SequenceEncodableProbabilityDistribution\n",
    "\n",
    "We first define a skeleton of the `SequenceEncodableProbabilityDistribution`. We know that the distribution requires parameters **μ**, **σ²**, and mixing weights **π**. These must be passed to the constructor. Note that the argument **name** must also be included. We won't get into the reason for this, but make sure to include it for consistency with other dmx-learn distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecd8df53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GmmDistribution(SequenceEncodableProbabilityDistribution):\n",
    "    \n",
    "    def __init__(self, mu: Union[Sequence[float], np.ndarray], sigma2: Union[Sequence[float], np.ndarray], w: Union[Sequence[float], np.ndarray], name: Optional[str] = None):\n",
    "        self.mu = np.asarray(mu)\n",
    "        self.sigma2 = np.asarray(sigma2)\n",
    "        self.w = np.asarray(w)\n",
    "        self.name = name\n",
    "\n",
    "        self.log_const = -0.5 * np.log(2.0 * np.pi)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return 'GmmDistribution(mu=%s, sigma2=%s, w=%s, name=%s)' % (repr(self.mu.tolist()), repr(self.sigma2.tolist()), repr(self.w.tolist()), repr(self.name))\n",
    "    \n",
    "    def log_density(self, x: float) -> float:\n",
    "        pass\n",
    "\n",
    "    def density(self, x: float) -> float:\n",
    "        return np.exp(self.log_density(x))\n",
    "\n",
    "    def seq_log_density(self, x) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def sampler(self, seed: Optional[int] = None):\n",
    "        pass\n",
    "    \n",
    "    def dist_to_encoder(self):\n",
    "        pass\n",
    "\n",
    "    def estimator(self, pseudo_count: Optional[float] = None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43cf68e",
   "metadata": {},
   "source": [
    "# Log Density of a Univariate Gaussian Mixture Model\n",
    "\n",
    "The next step is to define the log-likelihood in terms of the parameters set as member variables for the `SequenceEncodableProbabilityDistribution`. This is detailed below.\n",
    "\n",
    "A univariate Gaussian mixture model (GMM) can be expressed as:\n",
    "\n",
    "$$p(x) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x | \\mu_k, \\sigma_k^2)$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $K$ is the number of Gaussian components.\n",
    "- $\\pi_k$ is the mixing weight for component $k$ (with $\\sum_{k=1}^{K} \\pi_k = 1$).\n",
    "- $\\mathcal{N}(x | \\mu_k, \\sigma_k^2)$ is the Gaussian density function given by:\n",
    "\n",
    "$$\\mathcal{N}(x | \\mu_k, \\sigma_k^2) = \\frac{1}{\\sqrt{2\\pi \\sigma_k^2}} \\exp\\left(-\\frac{(x - \\mu_k)^2}{2\\sigma_k^2}\\right)$$\n",
    "\n",
    "To evaluate the log density of the GMM, we use the `logsumexp` trick to avoid numerical underflow or overflow when dealing with exponentials. The log density can be computed as follows:\n",
    "\n",
    "### 1. Compute the log densities for each component\n",
    "\n",
    "$$\\log p_k(x) = \\log \\pi_k + \\log \\mathcal{N}(x | \\mu_k, \\sigma_k^2)$$\n",
    "\n",
    "This expands to:\n",
    "\n",
    "$$\\log p_k(x) = \\log \\pi_k - \\frac{1}{2} \\log(2\\pi \\sigma_k^2) - \\frac{(x - \\mu_k)^2}{2\\sigma_k^2}$$\n",
    "\n",
    "### 2. Use `logsumexp` to compute the log density of the mixture\n",
    "\n",
    "The log density of the GMM can be computed as:\n",
    "\n",
    "$$\\log p(x) = \\log \\left( \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x | \\mu_k, \\sigma_k^2) \\right)$$\n",
    "\n",
    "Using the `logsumexp` function, we can rewrite this as:\n",
    "\n",
    "$$\\log p(x) = \\log \\left( \\sum_{k=1}^{K} \\exp\\left(\\log \\pi_k + \\log \\mathcal{N}(x | \\mu_k, \\sigma_k^2)\\right) \\right)$$\n",
    "\n",
    "This can be expressed as:\n",
    "\n",
    "$$\\log p(x) = \\log \\left( \\sum_{k=1}^{K} \\exp\\left(\\log \\pi_k - \\frac{1}{2} \\log(2\\pi \\sigma_k^2) - \\frac{(x - \\mu_k)^2}{2\\sigma_k^2}\\right) \\right)$$\n",
    "\n",
    "### 3. Final Expression\n",
    "\n",
    "Therefore, the log density of the univariate Gaussian mixture model can be computed using:\n",
    "\n",
    "$$\\log p(x) = \\log \\left( \\sum_{k=1}^{K} \\exp\\left(\\log \\pi_k - \\frac{1}{2} \\log(2\\pi \\sigma_k^2) - \\frac{(x - \\mu_k)^2}{2\\sigma_k^2}\\right) \\right)$$\n",
    "\n",
    "This formulation allows for stable computation of the log density of a Gaussian mixture model using the `logsumexp` trick, which is particularly useful in practice to avoid numerical issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "799edc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GmmDistribution(SequenceEncodableProbabilityDistribution):\n",
    "    \n",
    "    def __init__(self, mu: Union[Sequence[float], np.ndarray], sigma2: Union[Sequence[float], np.ndarray], w: Union[Sequence[float], np.ndarray], name: Optional[str] = None):\n",
    "        self.mu = np.asarray(mu)\n",
    "        self.sigma2 = np.asarray(sigma2)\n",
    "        self.w = np.asarray(w)\n",
    "        self.name = name\n",
    "\n",
    "        self.log_const = -0.5*np.log(2.0 * np.pi)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return 'GmmDistribution(mu=%s, sigma2=%s, w=%s, name=%s)' % (repr(self.mu.tolist()), repr(self.sigma2.tolist()), repr(self.w.tolist()), repr(self.name))\n",
    "    \n",
    "    def log_density(self, x: float) -> float:\n",
    "        # eval log-density for each component\n",
    "        ll = self.log_const - 0.50*(x-self.mu) ** 2 / self.sigma2 - 0.5*np.log(self.sigma2) + np.log(self.w)\n",
    "        max_ = np.max(ll)\n",
    "        # subtract max and exponentiate\n",
    "        np.exp(ll-max_, out=ll)\n",
    "        # finish log-sum-exp\n",
    "        rv = np.log(np.sum(ll)) + max_ \n",
    "        return rv\n",
    "\n",
    "    def density(self, x: float) -> float:\n",
    "        return np.exp(self.log_density(x))\n",
    "\n",
    "    def seq_log_density(self, x) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def sampler(self, seed: Optional[int] = None):\n",
    "        pass\n",
    "    \n",
    "    def dist_to_encoder(self):\n",
    "        pass\n",
    "\n",
    "    def estimator(self, pseudo_count: Optional[float] = None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88225802",
   "metadata": {},
   "source": [
    "# EncodedDataSequence and DataSequenceEncoder Objects\n",
    "\n",
    "To make calculations fast, we want to vectorize our `log_density` function call. Under the hood, this requires us to encode our data. In other words, we want to pre-process the data passed to our object so we can perform fast vectorized operations. The `DataSequenceEncoder` object is responsible for encoding our data into a format useful for repeated vectorized operations. The output data is stored in an `EncodedDataSequence` object. This object also allows for type checking if desired.\n",
    "\n",
    "A good way to understand how this works is to first consider a vectorized form of the `log_density` function. Assume the data is a one-dimensional numpy array (this is the form the encoded data will take). We can write out the density in vectorized form as seen below:\n",
    "\n",
    "```python\n",
    "def seq_log_density_(x: np.ndarray) -> np.ndarray:\n",
    "\n",
    "    ll = -0.5 * (x[:, None] - self.mu) ** 2 / self.sigma2\n",
    "    ll += - 0.5 * np.log(self.sigma2) + self.log_const + np.log(self.w)\n",
    "    max_ = np.max(ll, axis=1, keepdims=True)\n",
    "    np.exp(ll - max_, out=ll)\n",
    "    ll = np.log(np.sum(ll, axis=1, keepdims=False))\n",
    "    ll += max_.flatten()\n",
    "\n",
    "    return ll\n",
    "```\n",
    "\n",
    "The `EncodedDataSequence` should store the processed data (which happens to be a numpy array for floats). We can write that class as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf715781",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GmmEncodedDataSequence(EncodedDataSequence):\n",
    "\n",
    "    def __init__(self, data: np.ndarray):\n",
    "        super().__init__(data=data)\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f'GmmEncodedDataSequence(data={self.data})'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87231c2",
   "metadata": {},
   "source": [
    "Each model we create must have a `DataSequenceEncoder` which creates the `EncodedDataSequence`. The `DataSequenceEncoder` object must implement three methods: `__str__`, `__eq__`, and `seq_encode`. The `seq_encode` method should take the data and encode it, returning the result as an `EncodedDataSequence` object. Note that this is also the place to check for data compatibility (e.g., GMM can't handle NaN or inf values).\n",
    "\n",
    "The `__eq__` method is implemented to check if two `DataSequenceEncoder` objects are the same. This helps avoid multiple encodings under the hood when nesting `dmx-learn` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa6d33c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GmmDataEncoder(DataSequenceEncoder):\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return 'GmmDataEncoder'\n",
    "    \n",
    "    def __eq__(self, other) -> bool:\n",
    "        return isinstance(other, GmmDataEncoder)\n",
    "    \n",
    "    def seq_encode(self, x: Union[Sequence[float], np.ndarray]) -> 'GmmEncodedDataSequence':\n",
    "        rv = np.asarray(x, dtype=float)\n",
    "        \n",
    "        if np.any(np.isnan(rv)) or np.any(np.isinf(rv)):\n",
    "            raise Exception('GmmDistribution requires support x in (-inf,inf).')\n",
    "        \n",
    "        return GmmEncodedDataSequence(data=rv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e195c1e3",
   "metadata": {},
   "source": [
    "We can now fill in the `seq_log_density` function with proper type hints. We also fill out `dist_to_encoder`, which returns the appropriate `DataSequenceEncoder` object for encoding data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73d3e66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GmmDistribution(SequenceEncodableProbabilityDistribution):\n",
    "    \n",
    "    def __init__(self, mu: Union[Sequence[float], np.ndarray], sigma2: Union[Sequence[float], np.ndarray], w: Union[Sequence[float], np.ndarray], name: Optional[str] = None):\n",
    "        self.mu = np.asarray(mu)\n",
    "        self.sigma2 = np.asarray(sigma2)\n",
    "        self.w = np.asarray(w)\n",
    "        self.name = name\n",
    "\n",
    "        self.log_const = -0.5 * np.log(2.0 * np.pi)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return 'GmmDistribution(mu=%s, sigma2=%s, w=%s, name=%s)' % (repr(self.mu.tolist()), repr(self.sigma2.tolist()), repr(self.w.tolist()), repr(self.name))\n",
    "    \n",
    "    def log_density(self, x: float) -> float:\n",
    "        # eval log-density for each component\n",
    "        ll = self.log_const - 0.5 * (x - self.mu) ** 2 / self.sigma2 - 0.5 * np.log(self.sigma2) + np.log(self.w)\n",
    "        max_ = np.max(ll)\n",
    "        # subtract max and exponentiate\n",
    "        np.exp(ll - max_, out=ll)\n",
    "        # finish log-sum-exp\n",
    "        rv = np.log(np.sum(ll)) + max_ \n",
    "        return rv\n",
    "\n",
    "    def density(self, x: float) -> float:\n",
    "        return np.exp(self.log_density(x))\n",
    "\n",
    "    def seq_log_density(self, x: GmmEncodedDataSequence) -> np.ndarray:\n",
    "        # Type check\n",
    "        if not isinstance(x, GmmEncodedDataSequence):\n",
    "            raise Exception('GmmEncodedDataSequence requires for seq_log_density.')\n",
    "        \n",
    "        # Evaluate the vectorized log-density as before\n",
    "        ll = -0.5 * (x.data[:, None] - self.mu) ** 2 / self.sigma2 - 0.5 * np.log(self.sigma2) + self.log_const + np.log(self.w)\n",
    "        max_ = np.max(ll, axis=1, keepdims=True)\n",
    "        np.exp(ll - max_, out=ll)\n",
    "        ll = np.log(np.sum(ll, axis=1, keepdims=False))\n",
    "        ll += max_.flatten()\n",
    "\n",
    "        return ll\n",
    "    \n",
    "    def dist_to_encoder(self) -> GmmDataEncoder:\n",
    "        return GmmDataEncoder()\n",
    "    \n",
    "    def sampler(self, seed: Optional[int] = None):\n",
    "        pass\n",
    "\n",
    "    def estimator(self, pseudo_count: Optional[float] = None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05472a3",
   "metadata": {},
   "source": [
    "# DistributionSampler\n",
    "\n",
    "Next we create the `DistributionSampler`. The sampler allows us to draw samples from a fitted distribution. `DistributionSampler` objects are generally realized through the `sampler` method in the `SequenceEncodableProbabilityDistribution`.\n",
    "\n",
    "## Sampling the GMM\n",
    "\n",
    "The GMM sampling process follows these steps:\n",
    "\n",
    "1. **Draw a label from the mixture weights**: $z_i \\sim \\boldsymbol{\\pi}$\n",
    "2. **Sample an observation conditioned on the label drawn**: $x_i \\mid z_i = k \\sim \\mathcal{N}\\left(\\mu_k, \\sigma^2_k \\right)$\n",
    "\n",
    "Below is a vectorized implementation of GMM sampling in the `sample` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1396e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GmmSampler(DistributionSampler):\n",
    "\n",
    "    def __init__(self, dist: GmmDistribution, seed: Optional[int] = None):\n",
    "        self.rng = RandomState(seed)\n",
    "        self.dist = dist\n",
    "    \n",
    "    def sample(self, size: Optional[int] = None) -> Union[float, np.ndarray]:\n",
    "        ncomps = len(self.dist.w)\n",
    "        if size:\n",
    "            rv = np.zeros(size)\n",
    "            idx = np.arange(size)\n",
    "            self.rng.shuffle(idx)\n",
    "            z = self.rng.choice(ncomps, p=self.dist.w, replace=True, size=size)\n",
    "            z = np.bincount(z, minlength=ncomps)\n",
    "\n",
    "            i0 = 0\n",
    "            for xi, xc in enumerate(z):\n",
    "                if xc > 0:\n",
    "                    i1 = i0 + xc\n",
    "                    rv[idx[i0:i1]] = self.rng.normal(loc=self.dist.mu[xi], scale=np.sqrt(self.dist.sigma2[xi]), size=xc)\n",
    "                    i0 += xc\n",
    "                \n",
    "            return rv \n",
    "        else:\n",
    "            z = self.rng.choice(ncomps, p=self.dist.w)\n",
    "            rv = self.rng.randn() * np.sqrt(self.dist.sigma2[z]) + self.dist.mu[z]\n",
    "\n",
    "            return float(rv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710630c3",
   "metadata": {},
   "source": [
    "We can update the `sampler` method in `GmmDistribution`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd96bf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GmmDistribution(SequenceEncodableProbabilityDistribution):\n",
    "    \n",
    "    def __init__(self, mu: Union[Sequence[float], np.ndarray], sigma2: Union[Sequence[float], np.ndarray], w: Union[Sequence[float], np.ndarray], name: Optional[str] = None):\n",
    "        self.mu = np.asarray(mu)\n",
    "        self.sigma2 = np.asarray(sigma2)\n",
    "        self.w = np.asarray(w)\n",
    "        self.name = name\n",
    "\n",
    "        self.log_const = -0.5*np.log(2.0 * np.pi)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return 'GmmDistribution(mu=%s, sigma2=%s, w=%s, name=%s)' % (repr(self.mu.tolist()), repr(self.sigma2.tolist()), repr(self.w.tolist()), repr(self.name))\n",
    "    \n",
    "    def log_density(self, x: float) -> float:\n",
    "        # eval log-density for each component\n",
    "        ll = self.log_const - 0.5*(x-self.mu) ** 2 / self.sigma2 - 0.5*np.log(self.sigma2) + np.log(self.w)\n",
    "        max_ = np.max(ll)\n",
    "        # subtract max and exponentiate\n",
    "        np.exp(ll-max_, out=ll)\n",
    "        # finish log-sum-exp\n",
    "        rv = np.log(np.sum(ll)) + max_ \n",
    "        return rv\n",
    "\n",
    "    def density(self, x: float) -> float:\n",
    "        return np.exp(self.log_density(x))\n",
    "\n",
    "    def seq_log_density(self, x: GmmEncodedDataSequence) -> np.ndarray:\n",
    "        # Type check\n",
    "        if not isinstance(x, GmmEncodedDataSequence):\n",
    "            raise Exception('GmmEncodedDataSequence requires for seq_log_density.')\n",
    "        \n",
    "        # Evaluate the vetorized log-density as before\n",
    "        ll = -0.5*(x.data[:, None] - self.mu)**2 / self.sigma2 - 0.5*np.log(self.sigma2) + self.log_const + np.log(self.w)\n",
    "        max_ = np.max(ll, axis=1, keepdims=True)\n",
    "        np.exp(ll-max_, out=ll)\n",
    "        ll = np.log(np.sum(ll, axis=1, keepdims=False))\n",
    "        ll += max_.flatten()\n",
    "\n",
    "        return ll\n",
    "    \n",
    "    def dist_to_encoder(self) -> GmmDataEncoder:\n",
    "        return GmmDataEncoder()\n",
    "    \n",
    "    def sampler(self, seed: Optional[int] = None) -> GmmSampler:\n",
    "        return GmmSampler(dist=self, seed=seed)\n",
    "\n",
    "    def estimator(self, pseudo_count: Optional[float] = None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d248b5b1",
   "metadata": {},
   "source": [
    "We need to fill out the `estimator` method to complete the `GmmDistribution` class. We will need to write a `ParameterEstimator` class. I like to return to this later. \n",
    "\n",
    "The next step that I prefer to tackle is writing the `SequenceEncodableAccumulator`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662d8d81",
   "metadata": {},
   "source": [
    "# SequenceEncodableStatisticAccumulator\n",
    "\n",
    "Next we write the `SequenceEncodableStatisticAccumulator`, which is used to aggregate sufficient statistics. To identify the sufficient statistics and the calculations involved in tracking them, we can refer to the exponential family form of the distribution. In the case of the univariate GMM, it is easier to refer back to the E-step of the EM algorithm.\n",
    "\n",
    "## Expectation-Maximization Algorithm\n",
    "\n",
    "To estimate the parameters of a one-dimensional GMM, we typically use the Expectation-Maximization (EM) algorithm, which consists of two steps:\n",
    "\n",
    "### 1. Expectation Step (E-step)\n",
    "\n",
    "Calculate the responsibility (posterior probability) that component $k$ takes for data point $n$, given the current parameter estimates:\n",
    "\n",
    "$$\\gamma_{nk} = \\frac{\\pi_k \\mathcal{N}(x_n | \\mu_k, \\sigma_k^2)}{\\sum_{j=1}^{K} \\pi_j \\mathcal{N}(x_n | \\mu_j, \\sigma_j^2)}$$\n",
    "\n",
    "where $\\gamma_{nk}$ is the responsibility that component $k$ takes for data point $n$.\n",
    "\n",
    "### 2. Maximization Step (M-step)\n",
    "\n",
    "Update the parameters using the responsibilities computed in the E-step:\n",
    "\n",
    "**Mixture weights:**\n",
    "$$\\pi_k = \\frac{N_k}{N}$$\n",
    "\n",
    "**Component means:**\n",
    "$$\\mu_k = \\frac{1}{N_k} \\sum_{n=1}^{N} \\gamma_{nk} x_n$$\n",
    "\n",
    "**Component variances:**\n",
    "$$\\sigma_k^2 = \\frac{1}{N_k} \\sum_{n=1}^{N} \\gamma_{nk} (x_n - \\mu_k)^2$$\n",
    "\n",
    "where $N_k = \\sum_{n=1}^{N} \\gamma_{nk}$ is the effective number of points assigned to component $k$.\n",
    "\n",
    "## Sufficient Statistics\n",
    "\n",
    "In dmx-learn, the accumulator tracks the sufficient statistics, which are used to perform the estimation step. From the M-step equations above, we see that three sufficient statistics are required to estimate $\\pi_k$, $\\mu_k$, and $\\sigma^2_k$:\n",
    "\n",
    "$$\\sum_{n=1}^{N} \\gamma_{nk}$$\n",
    "\n",
    "$$\\sum_{n=1}^{N} \\gamma_{nk} x_n$$\n",
    "\n",
    "$$\\sum_{n=1}^{N} \\gamma_{nk} x_n^2$$\n",
    "\n",
    "Our accumulator class will aggregate these sufficient statistics. We denote them with member variables: \n",
    "- `comp_counts` = $\\sum_{n=1}^{N} \\gamma_{nk}$\n",
    "- `x` = $\\sum_{n=1}^{N} \\gamma_{nk} x_n$\n",
    "- `x2` = $\\sum_{n=1}^{N} \\gamma_{nk} x_n^2$\n",
    "\n",
    "Note that each of these member variables are $k$-dimensional vectors, where $k$ is the number of mixture components.\n",
    "\n",
    "## Accumulator Skeleton\n",
    "\n",
    "Below is a skeleton of the `SequenceEncodableStatisticAccumulator`. The methods `value`, `combine`, and `from_value` must all be implemented:\n",
    "- **`value`**: Returns the sufficient statistics from the accumulator\n",
    "- **`combine`**: Combines the current sufficient statistics with another set of sufficient statistics\n",
    "- **`from_value`**: Assigns the accumulator instance sufficient statistics from a given value\n",
    "\n",
    "### Key Functionality\n",
    "\n",
    "An interesting feature is the `keys` parameter. For the Gaussian Mixture model implementation, we allow the user to pass keys specifying whether the mixture weights or means and variances should be shared across other distributions (with matching keys). You must implement two methods:\n",
    "- **`key_merge`**: Merges sufficient statistics with matching keys\n",
    "- **`key_replace`**: Replaces sufficient statistics with values from matching keys\n",
    "\n",
    "The member function `acc_to_encoder` is similar to `dist_to_encoder` from the distribution class. It must be included here, as we require data encodings to be available in the initialization step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3376f5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GmmAccumulator(SequenceEncodableStatisticAccumulator):\n",
    "\n",
    "    def __init__(self, num_comps: int, keys: Optional[Tuple[Optional[str], Optional[str]]] = None, name: Optional[str] = None):\n",
    "        self.x = np.zeros(num_comps)\n",
    "        self.x2 = np.zeros(num_comps)\n",
    "        self.comp_counts = np.zeros(num_comps)\n",
    "        self.ncomps = num_comps\n",
    "        self.weight_keys = keys[0] if keys else None\n",
    "        self.param_keys = keys[1] if keys else None\n",
    "\n",
    "    def initialize(self, x: float, weight: float, rng: Optional[RandomState]):\n",
    "        pass\n",
    "\n",
    "    def update(self, x: float, weight: float, estimate: GmmDistribution):\n",
    "        pass\n",
    "\n",
    "    def seq_initialize(self, x: GmmEncodedDataSequence, weights: np.ndarray, rng: Optional[RandomState]):\n",
    "        pass\n",
    "\n",
    "    def seq_update(self, x: GmmEncodedDataSequence, weights: np.ndarray, estimate: GmmDistribution):\n",
    "        pass\n",
    "\n",
    "    # Return the sufficient statistics\n",
    "    def value(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        return self.comp_counts, self.x, self.x2\n",
    "    \n",
    "    # Combine suff stats with the accumulators\n",
    "    def combine(self, x: Tuple[np.ndarray, np.ndarray, np.ndarray]):\n",
    "        self.comp_counts += x[0]\n",
    "        self.x += x[1]\n",
    "        self.x2 += x[2]\n",
    "\n",
    "        return self\n",
    "    \n",
    "    # assign sufficient statistics from a value\n",
    "    def from_value(self, x: Tuple[np.ndarray, np.ndarray, np.ndarray]):\n",
    "        self.comp_counts = x[0]\n",
    "        self.x = x[1]\n",
    "        self.x2 = x[2]\n",
    "\n",
    "    # This allows for merging of suff stats with parameters that have the same keys\n",
    "    def key_merge(self, stats_dict: Dict[str, Any]):\n",
    "        if self.weight_keys is not None:\n",
    "            if self.weight_keys in stats_dict:\n",
    "                self.comp_counts += stats_dict[self.weight_keys]\n",
    "            else:\n",
    "                stats_dict[self.weight_keys] = self.comp_counts\n",
    "        \n",
    "        if self.param_keys is not None:\n",
    "            if self.param_keys in stats_dict:\n",
    "                x, x2 = stats_dict[self.param_keys]\n",
    "                self.x += x\n",
    "                self.x2 += x2\n",
    "            else:\n",
    "                stats_dict[self.param_keys] = (self.x, self.x2)\n",
    "\n",
    "    # Set the sufficient statistics of the accumulator to suff stats with matching keys.\n",
    "    def key_replace(self, stats_dict: Dict[str, Any]):\n",
    "        if self.weight_keys is not None:\n",
    "            if self.weight_keys in stats_dict:\n",
    "                self.comp_counts = stats_dict[self.weight_keys]\n",
    "\n",
    "        if self.param_keys is not None:\n",
    "            if self.param_keys in stats_dict:\n",
    "                self.param_keys = stats_dict[self.param_keys]\n",
    "    \n",
    "    # Create a DataSequenceEncoder object for seq initialize encodings.\n",
    "    def acc_to_encoder(self):\n",
    "        return GmmDataEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1e4bcf",
   "metadata": {},
   "source": [
    "# Implementing Update\n",
    "\n",
    "**Recall the Expectation Step (E-step):** Calculate the responsibility (posterior probability) that component $k$ takes for data point $n$, given the current parameter estimates:\n",
    "\n",
    "$$\\gamma_{nk} = \\frac{\\pi_k \\mathcal{N}(x_n | \\mu_k, \\sigma_k^2)}{\\sum_{j=1}^{K} \\pi_j \\mathcal{N}(x_n | \\mu_j, \\sigma_j^2)}$$\n",
    "\n",
    "where $\\gamma_{nk}$ is the responsibility that component $k$ takes for data point $n$.\n",
    "\n",
    "For the `update` function, we must calculate the posterior $\\gamma_{nk}$ for each observation $x_n$. This is done using the log-sum-exp trick. Once we have $\\gamma_{nk}$, we simply update the accumulator's sufficient statistics `x`, `x2`, and `comp_counts` accordingly. Note that `weight` is also multiplied to the $\\gamma_{nk}$ values, as this allows for nesting with other dmx-learn classes.\n",
    "\n",
    "We must also implement the vectorized `seq_update`, which takes the `GmmEncodedDataSequence` previously defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7b31464",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GmmAccumulator(SequenceEncodableStatisticAccumulator):\n",
    "\n",
    "    def __init__(self, num_comps: int, keys: Optional[Tuple[Optional[str], Optional[str]]] = None, name: Optional[str] = None):\n",
    "        self.x = np.zeros(num_comps)\n",
    "        self.x2 = np.zeros(num_comps)\n",
    "        self.comp_counts = np.zeros(num_comps)\n",
    "        self.ncomps = num_comps\n",
    "        self.weight_keys = keys[0] if keys else None\n",
    "        self.param_keys = keys[1] if keys else None\n",
    "\n",
    "    def update(self, x: float, weight: float, estimate: GmmDistribution):\n",
    "        mu, s2, w = estimate.mu, estimate.sigma2, estimate.w\n",
    "\n",
    "        gamma = -0.5*(x-mu)**2 / s2 - 0.5*np.log(s2) + np.log(w)\n",
    "        max_ = np.max(gamma)\n",
    "\n",
    "        if not np.isinf(max_):\n",
    "            # log-sum-exp back to exp\n",
    "            gamma = np.exp(gamma-max_, out=gamma)\n",
    "            gamma /= np.sum(gamma)\n",
    "            # multiply by weight to allow for down stream nesting with other dmx classes\n",
    "            gamma *= weight\n",
    "            self.comp_counts += gamma\n",
    "            self.x += x*gamma\n",
    "            self.x2 += x**2*gamma \n",
    "\n",
    "    def seq_update(self, x: GmmEncodedDataSequence, weights: np.ndarray, estimate: GmmDistribution):\n",
    "        mu, s2, log_w = estimate.mu, estimate.sigma2, np.log(estimate.w)\n",
    "        gammas = -0.5*(x.data[:, None] - mu)**2 / s2 - 0.5*np.log(s2)\n",
    "        gammas += log_w[None, :]\n",
    "\n",
    "        # check for 0 weights\n",
    "        zw = np.isinf(log_w)\n",
    "        if np.any(zw):\n",
    "            gammas[:, zw] = -np.inf\n",
    "        \n",
    "        max_ = np.max(gammas, axis=1, keepdims=True)\n",
    "\n",
    "        # correct for any posterior containing all -np.inf values.\n",
    "        bad_rows = np.all(np.isinf(gammas), axis=1).flatten()\n",
    "        gammas[bad_rows, :] = log_w.copy()\n",
    "        max_[bad_rows] = np.max(log_w)\n",
    "\n",
    "        # logsumexp and multiply by weights passed \n",
    "        gammas -= max_\n",
    "        np.exp(gammas, out=gammas)\n",
    "        np.sum(gammas, axis=1, keepdims=True, out=max_)\n",
    "        np.divide(weights[:, None], max_, out=max_)\n",
    "        gammas *= max_\n",
    "\n",
    "        # update the sufficient stats\n",
    "        wsum = gammas.sum(axis=0)\n",
    "        self.comp_counts += wsum\n",
    "        self.x += np.dot(x.data, gammas)\n",
    "        self.x2 += np.dot(x.data**2, gammas)\n",
    "\n",
    "    def initialize(self, x: float, weight: float, rng: Optional[RandomState]):\n",
    "        pass\n",
    "\n",
    "    def seq_initialize(self, x: GmmEncodedDataSequence, weights: np.ndarray, rng: Optional[RandomState]):\n",
    "        pass\n",
    "\n",
    "    # Return the sufficient statistics\n",
    "    def value(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        return self.comp_counts, self.x, self.x2\n",
    "    \n",
    "    # Combine suff stats with the accumulators\n",
    "    def combine(self, x: Tuple[np.ndarray, np.ndarray, np.ndarray]):\n",
    "        self.comp_counts += x[0]\n",
    "        self.x += x[1]\n",
    "        self.x2 += x[2]\n",
    "\n",
    "        return self\n",
    "    \n",
    "    # assign sufficient statistics from a value\n",
    "    def from_value(self, x: Tuple[np.ndarray, np.ndarray, np.ndarray]):\n",
    "        self.comp_counts = x[0]\n",
    "        self.x = x[1]\n",
    "        self.x2 = x[2]\n",
    "\n",
    "    # This allows for merging of suff stats with parameters that have the same keys\n",
    "    def key_merge(self, stats_dict: Dict[str, Any]):\n",
    "        if self.weight_keys is not None:\n",
    "            if self.weight_keys in stats_dict:\n",
    "                self.comp_counts += stats_dict[self.weight_keys]\n",
    "            else:\n",
    "                stats_dict[self.weight_keys] = self.comp_counts\n",
    "        \n",
    "        if self.param_keys is not None:\n",
    "            if self.param_keys in stats_dict:\n",
    "                x, x2 = stats_dict[self.param_keys]\n",
    "                self.x += x\n",
    "                self.x2 += x2\n",
    "            else:\n",
    "                stats_dict[self.param_keys] = (self.x, self.x2)\n",
    "\n",
    "    # Set the sufficient statistics of the accumulator to suff stats with matching keys.\n",
    "    def key_replace(self, stats_dict: Dict[str, Any]):\n",
    "        if self.weight_keys is not None:\n",
    "            if self.weight_keys in stats_dict:\n",
    "                self.comp_counts = stats_dict[self.weight_keys]\n",
    "\n",
    "        if self.param_keys is not None:\n",
    "            if self.param_keys in stats_dict:\n",
    "                self.param_keys = stats_dict[self.param_keys]\n",
    "    \n",
    "    # Create a DataSequenceEncoder object for seq initialize encodings.\n",
    "    def acc_to_encoder(self):\n",
    "        return GmmDataEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e17d97",
   "metadata": {},
   "source": [
    "# Initialize\n",
    "\n",
    "The sufficient statistics must be initialized. The `SequenceEncodableStatisticAccumulator` object allows for randomized initialization of the sufficient statistics for the observed data. The methods required here are `initialize` and the vectorized version `seq_initialize`. It is up to you to define how these methods are implemented. Below we outline the method for the univariate Gaussian mixture model.\n",
    "\n",
    "## Initialization of GMM Sufficient Statistics\n",
    "\n",
    "The initialization process follows these steps:\n",
    "\n",
    "1. **Draw random responsibilities from a Dirichlet distribution:**\n",
    "   $$\\boldsymbol{\\gamma}_i \\sim \\text{Dirichlet}\\left(\\left( \\frac{1}{k}, \\frac{1}{k}, \\ldots, \\frac{1}{k} \\right)\\right)$$\n",
    "\n",
    "2. **Multiply by the passed `weight`** (this enables nesting with other dmx-learn distributions):\n",
    "   $$\\boldsymbol{\\gamma}_i = \\boldsymbol{\\gamma}_i \\times \\text{weight}_i$$\n",
    "\n",
    "3. **Update the sufficient statistic member variables** of the accumulator:\n",
    "   $$\\text{comp\\_counts}[k] \\text{ += } \\gamma_{i, k}$$\n",
    "   $$\\text{x}[k] \\text{ += } \\gamma_{i,k} \\times x_i$$\n",
    "   $$\\text{x2}[k] \\text{ += } \\gamma_{i,k} \\times x^2_i$$\n",
    "\n",
    "This is straightforward to vectorize and implement in `seq_initialize` using our encoded data previously defined as `GmmEncodedDataSequence`. The code is provided below. \n",
    "\n",
    "**Note:** The value `c` is used in the initialization to avoid numerical issues when sampling from a Dirichlet distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe75e6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GmmAccumulator(SequenceEncodableStatisticAccumulator):\n",
    "\n",
    "    def __init__(self, num_comps: int, keys: Optional[Tuple[Optional[str], Optional[str]]] = None, name: Optional[str] = None):\n",
    "        self.x = np.zeros(num_comps)\n",
    "        self.x2 = np.zeros(num_comps)\n",
    "        self.comp_counts = np.zeros(num_comps)\n",
    "        self.ncomps = num_comps\n",
    "        self.weight_keys = keys[0] if keys else None\n",
    "        self.param_keys = keys[1] if keys else None\n",
    "\n",
    "    def update(self, x: float, weight: float, estimate: GmmDistribution):\n",
    "        mu, s2, w = estimate.mu, estimate.sigma2, estimate.w\n",
    "\n",
    "        gamma = -0.5*(x-mu)**2 / s2 - 0.5*np.log(s2) + np.log(w)\n",
    "        max_ = np.max(gamma)\n",
    "\n",
    "        if not np.isinf(max_):\n",
    "            # log-sum-exp back to exp\n",
    "            gamma = np.exp(gamma-max_, out=gamma)\n",
    "            gamma /= np.sum(gamma)\n",
    "            # multiply by weight to allow for down stream nesting with other dmx classes\n",
    "            gamma *= weight\n",
    "            self.comp_counts += gamma\n",
    "            self.x += x*gamma\n",
    "            self.x2 += x**2*gamma \n",
    "\n",
    "    def seq_update(self, x: GmmEncodedDataSequence, weights: np.ndarray, estimate: GmmDistribution):\n",
    "        mu, s2, log_w = estimate.mu, estimate.sigma2, np.log(estimate.w)\n",
    "        gammas = -0.5*(x.data[:, None] - mu)**2 / s2 - 0.5*np.log(s2)\n",
    "        gammas += log_w[None, :]\n",
    "\n",
    "        # check for 0 weights\n",
    "        zw = np.isinf(log_w)\n",
    "        if np.any(zw):\n",
    "            gammas[:, zw] = -np.inf\n",
    "        \n",
    "        max_ = np.max(gammas, axis=1, keepdims=True)\n",
    "\n",
    "        # correct for any posterior containing all -np.inf values.\n",
    "        bad_rows = np.all(np.isinf(gammas), axis=1).flatten()\n",
    "        gammas[bad_rows, :] = log_w.copy()\n",
    "        max_[bad_rows] = np.max(log_w)\n",
    "\n",
    "        # logsumexp and multiply by weights passed \n",
    "        gammas -= max_\n",
    "        np.exp(gammas, out=gammas)\n",
    "        np.sum(gammas, axis=1, keepdims=True, out=max_)\n",
    "        np.divide(weights[:, None], max_, out=max_)\n",
    "        gammas *= max_\n",
    "\n",
    "        # update the sufficient stats\n",
    "        wsum = gammas.sum(axis=0)\n",
    "        self.comp_counts += wsum\n",
    "        self.x += np.dot(x.data, gammas)\n",
    "        self.x2 += np.dot(x.data**2, gammas)\n",
    "\n",
    "    def initialize(self, x: float, weight: float, rng: RandomState):\n",
    "\n",
    "        # generate random posterior values\n",
    "        c = 20 ** 2 if self.ncomps > 20 else self.ncomps**2\n",
    "        ww = rng.dirichelt(np.ones(self.ncomps) / c)\n",
    "        ww *= weight\n",
    "\n",
    "        # update suff stats\n",
    "        self.x += x * ww\n",
    "        self.x2 += x**2 * ww\n",
    "        self.comp_counts += ww\n",
    "\n",
    "\n",
    "    def seq_initialize(self, x: GmmEncodedDataSequence, weights: np.ndarray, rng: Optional[RandomState]):\n",
    "\n",
    "        # only generate random posteriors for weights that are non-zero\n",
    "        sz = len(weights)\n",
    "        c = 20 ** 2 if self.ncomps > 20 else self.ncomps ** 2\n",
    "\n",
    "        ww = rng.dirichlet(np.ones(self.ncomps) / c, size=sz)\n",
    "        ww *= weights[:, None]\n",
    "        w_sum = ww.sum(axis=0)\n",
    "\n",
    "        # initialize suff stats\n",
    "        self.comp_counts += w_sum\n",
    "        self.x += np.dot(x.data, ww)\n",
    "        self.x2 += np.dot(x.data ** 2, ww)\n",
    "\n",
    "    # Return the sufficient statistics\n",
    "    def value(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        return self.comp_counts, self.x, self.x2\n",
    "    \n",
    "    # Combine suff stats with the accumulators\n",
    "    def combine(self, x: Tuple[np.ndarray, np.ndarray, np.ndarray]):\n",
    "        self.comp_counts += x[0]\n",
    "        self.x += x[1]\n",
    "        self.x2 += x[2]\n",
    "\n",
    "        return self\n",
    "    \n",
    "    # assign sufficient statistics from a value\n",
    "    def from_value(self, x: Tuple[np.ndarray, np.ndarray, np.ndarray]):\n",
    "        self.comp_counts = x[0]\n",
    "        self.x = x[1]\n",
    "        self.x2 = x[2]\n",
    "\n",
    "    # This allows for merging of suff stats with parameters that have the same keys\n",
    "    def key_merge(self, stats_dict: Dict[str, Any]):\n",
    "        if self.weight_keys is not None:\n",
    "            if self.weight_keys in stats_dict:\n",
    "                self.comp_counts += stats_dict[self.weight_keys]\n",
    "            else:\n",
    "                stats_dict[self.weight_keys] = self.comp_counts\n",
    "        \n",
    "        if self.param_keys is not None:\n",
    "            if self.param_keys in stats_dict:\n",
    "                x, x2 = stats_dict[self.param_keys]\n",
    "                self.x += x\n",
    "                self.x2 += x2\n",
    "            else:\n",
    "                stats_dict[self.param_keys] = (self.x, self.x2)\n",
    "\n",
    "    # Set the sufficient statistics of the accumulator to suff stats with matching keys.\n",
    "    def key_replace(self, stats_dict: Dict[str, Any]):\n",
    "        if self.weight_keys is not None:\n",
    "            if self.weight_keys in stats_dict:\n",
    "                self.comp_counts = stats_dict[self.weight_keys]\n",
    "\n",
    "        if self.param_keys is not None:\n",
    "            if self.param_keys in stats_dict:\n",
    "                self.param_keys = stats_dict[self.param_keys]\n",
    "    \n",
    "    # Create a DataSequenceEncoder object for seq initialize encodings.\n",
    "    def acc_to_encoder(self):\n",
    "        return GmmDataEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df58b834",
   "metadata": {},
   "source": [
    "# StatisticAccumulatorFactory\n",
    "\n",
    "\n",
    "In programming, a factory object is a design pattern used to create instances of objects. Instead of calling a constructor directly to create an object, a factory provides a method that returns an instance of a class. We define the `StatisticAccumulatorFactory` as a method for creating `SequenceEncodableStatisticAccumulator` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c3134cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GmmAccumulatorFactory(StatisticAccumulatorFactory):\n",
    "\n",
    "    # same constructor as the GmmAccumulator object\n",
    "    def __init__(self, num_comps: int, keys: Optional[Tuple[Optional[str], Optional[str]]] = None, name: Optional[str] = None):\n",
    "        self.num_comps = num_comps\n",
    "        self.keys = keys\n",
    "        self.name = name\n",
    "\n",
    "    # creates a GmmAccumulator object \n",
    "    def make (self) -> 'GmmAccumulator':\n",
    "        return GmmAccumulator(num_comps=self.num_comps, keys=self.keys, name=self.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d081922",
   "metadata": {},
   "source": [
    "The only thing left to do is to implement the `ParameterEstimator` class and fill out the `estimator` function call in the `SequenceEncodableProbabilityDistribution`. So let's implement the `ParameterEstimator` and tie everything together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18a9e7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GmmEstimator(ParameterEstimator):\n",
    "\n",
    "    def __init__(self, num_comps: int, suff_stat: Tuple[Optional[np.ndarray], Optional[np.ndarray], Optional[np.ndarray]] = (None, None, None), pseudo_count: Tuple[Optional[float], Optional[float], Optional[float]] = (None, None, None), keys: Tuple[Optional[str], Optional[str]] = (None, None)):\n",
    "        self.ncomps = num_comps\n",
    "        self.suff_stat = suff_stat\n",
    "        self.pseudo_count = pseudo_count\n",
    "        self.keys = keys\n",
    "\n",
    "    def accumulator_factory(self) -> GmmAccumulatorFactory:\n",
    "        return GmmAccumulatorFactory(num_comps=self.ncomps, keys=self.keys)\n",
    "    \n",
    "    def estimate(self, nobs: Optional[float], suff_stat: Tuple[np.ndarray, np.ndarray, np.ndarray]) -> GmmDistribution:\n",
    "        counts, xw, x2w = suff_stat\n",
    "\n",
    "        # regularize weights without suff stat passed to it\n",
    "        if self.pseudo_count[0] and not self.suff_stat[0]:\n",
    "            p = self.pseudo_count[0] / self.ncomps\n",
    "            w = counts + p\n",
    "            w /= w.sum()\n",
    "        # regularize weights with suff stat passed\n",
    "        elif self.pseudo_count[0] and self.suff_stat[0]:\n",
    "            w = (counts + self.suff_stat[0]*self.pseudo_count[0]) / (counts.sum() + self.pseudo_count[0]*self.suff_stat[0].sum())\n",
    "        # dont regularize weights\n",
    "        else:\n",
    "            wsum = counts.sum()\n",
    "\n",
    "            if wsum == 0.0:\n",
    "                w = np.ones(self.ncomps) / float(self.ncomps)\n",
    "            else:\n",
    "                w = counts.copy() / wsum\n",
    "\n",
    "        # flatten the mean estimates\n",
    "        if self.pseudo_count[1] is not None and self.suff_stat[1] is not None:\n",
    "            mu = (xw + self.pseudo_count[1] * self.suff_stat[1]) / (counts + self.pseudo_count * np.sum(self.suff_stats[1]))\n",
    "        else:\n",
    "            wsum = counts.copy()\n",
    "            wsum[wsum==0.0] = 1.0\n",
    "            mu = xw / wsum\n",
    "        \n",
    "        # flatten/regularize the variance estimates\n",
    "        if self.pseudo_count[2] and self.suff_stat[2]:\n",
    "            s2 = (x2w - mu**2 * counts * self.pseudo_count[2] * self.suff_stat[2]) / (counts + self.pseudo_count[2] * np.sum(self.suff_stat[2]))\n",
    "        else:\n",
    "            wsum = counts.copy()\n",
    "            wsum[wsum==0.0] = 1.0\n",
    "            s2 = x2w / wsum - mu * mu \n",
    "\n",
    "        return GmmDistribution(mu=mu, sigma2=s2, w=w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708d62e4",
   "metadata": {},
   "source": [
    "We can now fill out `estimate` in the `SequenceEncodableProbabilityDistribution`. This completes the GMM class for use in `dmx-learn`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38536b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GmmDistribution(SequenceEncodableProbabilityDistribution):\n",
    "    \n",
    "    def __init__(self, mu: Union[Sequence[float], np.ndarray], sigma2: Union[Sequence[float], np.ndarray], w: Union[Sequence[float], np.ndarray], name: Optional[str] = None):\n",
    "        self.mu = np.asarray(mu)\n",
    "        self.sigma2 = np.asarray(sigma2)\n",
    "        self.w = np.asarray(w)\n",
    "        self.name = name\n",
    "\n",
    "        self.log_const = -0.5*np.log(2.0 * np.pi)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return 'GmmDistribution(mu=%s, sigma2=%s, w=%s, name=%s)' % (repr(self.mu.tolist()), repr(self.sigma2.tolist()), repr(self.w.tolist()), repr(self.name))\n",
    "    \n",
    "    def log_density(self, x: float) -> float:\n",
    "        # eval log-density for each component\n",
    "        ll = self.log_const - 0.5*(x-self.mu) ** 2 / self.sigma2 - 0.5*np.log(self.sigma2) + np.log(self.w)\n",
    "        max_ = np.max(ll)\n",
    "        # subtract max and exponentiate\n",
    "        np.exp(ll-max_, out=ll)\n",
    "        # finish log-sum-exp\n",
    "        rv = np.log(np.sum(ll)) + max_ \n",
    "        return rv\n",
    "\n",
    "    def density(self, x: float) -> float:\n",
    "        return np.exp(self.log_density(x))\n",
    "\n",
    "    def seq_log_density(self, x: GmmEncodedDataSequence) -> np.ndarray:\n",
    "        # Type check\n",
    "        if not isinstance(x, GmmEncodedDataSequence):\n",
    "            raise Exception('GmmEncodedDataSequence requires for seq_log_density.')\n",
    "        \n",
    "        # Evaluate the vetorized log-density as before\n",
    "        ll = -0.5*(x.data[:, None] - self.mu)**2 / self.sigma2 + self.log_const + np.log(self.w) - 0.5*np.log(self.sigma2)\n",
    "        max_ = np.max(ll, axis=1, keepdims=True)\n",
    "        np.exp(ll-max_, out=ll)\n",
    "        ll = np.log(np.sum(ll, axis=1, keepdims=False))\n",
    "        ll += max_.flatten()\n",
    "\n",
    "        return ll\n",
    "    \n",
    "    def dist_to_encoder(self) -> GmmDataEncoder:\n",
    "        return GmmDataEncoder()\n",
    "    \n",
    "    def sampler(self, seed: Optional[int] = None) -> GmmSampler:\n",
    "        return GmmSampler(dist=self, seed=seed)\n",
    "\n",
    "    def estimator(self, pseudo_count: Optional[float] = None):\n",
    "        pc = (pseudo_count, pseudo_count, pseudo_count)\n",
    "        return GmmEstimator(num_comps=len(self.w), pseudo_count=pc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423640f7",
   "metadata": {},
   "source": [
    "# Proof of Concept\n",
    "Let's walk through the standard dmx-learn pipeline. First we declare the model and simulate some data. We then declare the estimator and fit the model using `optimize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23fd570f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100: ln[p_mat(Data|Model)]=-2.856283e+03, ln[p_mat(Data|Model)]-ln[p_mat(Data|PrevModel)]=1.894797e-02\n",
      "Iteration 200: ln[p_mat(Data|Model)]=-2.533313e+03, ln[p_mat(Data|Model)]-ln[p_mat(Data|PrevModel)]=2.064315e-04\n",
      "Iteration 210: ln[p_mat(Data|Model)]=-2.533313e+03, ln[p_mat(Data|Model)]-ln[p_mat(Data|PrevModel)]=3.501555e-10\n"
     ]
    }
   ],
   "source": [
    "N = 1000\n",
    "k = 3\n",
    "w = np.ones(k) / float(k)\n",
    "mu = np.linspace(-5, 5, k)\n",
    "sigma2 = np.ones(k) / 1.\n",
    "\n",
    "dist = GmmDistribution(mu=mu, sigma2=sigma2, w=w)\n",
    "\n",
    "sampler = dist.sampler(seed=1)\n",
    "data = sampler.sample(N)\n",
    "\n",
    "est = GmmEstimator(num_comps=k)\n",
    "fit = optimize(data=data, estimator=est, max_its=10000, print_iter=100, rng=RandomState(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4e8682ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w=[0.3319333291055791, 0.34082649223097533, 0.3272401786634457]\n",
      "mu=[0.03013664825881224, 4.974043219942388, -5.007351898485827]\n",
      "sigma2=[1.1102331677970434, 0.9933375732503364, 1.1840310643626815]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join([f\"w={fit.w.tolist()}\", f\"mu={fit.mu.tolist()}\", f\"sigma2={fit.sigma2.tolist()}\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "43604171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w=[0.33333333 0.33333333 0.33333333]\n",
      "mu=[-5.  0.  5.]\n",
      "sigma2=[1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join([f\"w={w}\", f\"mu={mu}\", f\"sigma2={sigma2}\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34da9140",
   "metadata": {},
   "source": [
    "This wraps things up. Keep in mind you are free to add other member functions to the ``SequenceEncodableProbabilityDistribution`` class that improve your quality of life. One thing you can try out is implementing a ``posterior`` and vectorized version ``seq_posterior`` that computes the posterior probability of component membership.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DMX 1.0.0 (dmx_kernel)",
   "language": "python",
   "name": "dmx_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
