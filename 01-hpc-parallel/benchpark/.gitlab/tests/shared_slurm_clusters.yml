# Clusters listed here are native slurm clusters that we test using flux. See "non_shared.yaml"
include:
  - local: .gitlab/utils/status.yml
  - local: .gitlab/utils/machine_checks.yml
  - local: .gitlab/utils/rules.yml
workflow:
  auto_cancel:
    on_new_commit: conservative
#############################
# Resource Allocation Section
#############################
.allocate_resources_slurm:
  extends: .on_host_template
  stage: allocate-resources
  script:
    - set -e
    # submit and grab JobID
    - JOBID=$(sbatch --parsable ${SHARED_ALLOC} --wait-all-nodes=1 --job-name="${ALLOC_NAME}"
      --wrap='srun -N 1 -n 1 flux start sleep inf')
    - echo "Slurm job = ${JOBID}"
    # wait until SLURM job running
    # 1..2160 is up to 6 hours
    - for i in {1..2160}; do state=$(sacct -j ${JOBID} -n -o State 2>/dev/null | head
      -1 | awk '{print $1}'); [[ "$state" == "RUNNING" ]] && break; echo "waiting...
      total duration has been $(( (i-1)*10 ))s" && sleep 10; done
    #- for i in {1..1080}; do flux proxy slurm:${JOBID} flux ping -c1 >/dev/null 2>&1 && break; echo "waiting..." sleep 10; done
    #- flux proxy slurm:${JOBID} flux ping -c1  # final check (fails the job if still not ready)
  after_script:
    - |
      if [[ "$CI_JOB_STATUS" == "canceled" ]]; then
        . .gitlab/utils/cancel-slurm.sh
      fi
allocate_resources_dane:
  extends: .allocate_resources_slurm
  variables:
    HOST: dane
    SHARED_ALLOC: $DANE_SHARED_ALLOC
  tags: [shell, dane]
  needs: [dane-up-check]
allocate_resources_matrix:
  extends: .allocate_resources_slurm
  variables:
    HOST: matrix
    SHARED_ALLOC: $MATRIX_SHARED_ALLOC
  tags: [shell, matrix]
  needs: [matrix-up-check]
##########################
# Resource Release Section
##########################
.release_resources_slurm:
  extends: .on_host_template
  stage: release-resources
  # Do not cleanup dir yet
  script: [. .gitlab/utils/cancel-slurm.sh --no-clean]
release_resources_dane:
  extends: .release_resources_slurm
  tags: [shell, dane]
  needs: [allocate_resources_dane, run_tests_slurm_dane]
release_resources_matrix:
  extends: .release_resources_slurm
  tags: [shell, matrix]
  needs: [allocate_resources_matrix, run_tests_slurm_matrix]
##################
# Test Run Section
##################
.run_tests_slurm:
  extends: .on_host_template
  stage: test
  before_script:
    - !reference [.report_status, before_script]
  script:
    - |
      echo -e "### Allocation name is ${ALLOC_NAME}"
      export JOBID=$(squeue -h --name=${ALLOC_NAME} --format=%A)
      echo -e "### Job ID is ${JOBID}"
    - |
      PROXY="$( [[ -n "${JOBID}" ]] && echo "flux proxy slurm:${JOBID}" || echo "" )"
      ${PROXY} flux watch $( ${PROXY} flux batch -o output.stdout.type=kvs ${FLUX_ARGS} .gitlab/utils/run-experiment.sh )
  after_script:
    - !reference [.report_status, after_script]
    - |
      if [[ "$CI_JOB_STATUS" == "canceled" ]]; then
        . .gitlab/utils/cancel-slurm.sh
      fi
run_tests_slurm_dane:
  extends: .run_tests_slurm
  tags: [shell, dane]
  needs: [allocate_resources_dane]
  parallel:
    matrix:
      - HOST: dane
        ARCHCONFIG: llnl-cluster
        BENCHMARK: [hpl, hpcg, amg2023, kripke, sparta-snl]
        VARIANT: [+openmp, '']
      - HOST: dane
        ARCHCONFIG: llnl-cluster
        BENCHMARK: [laghos, lammps, raja-perf, phloem]
      # Strong scaling runs for "benchpark analyze" test
      - HOST: dane
        ARCHCONFIG: llnl-cluster
        BENCHMARK: [kripke, amg2023]
        VARIANT: ['+strong caliper=mpi,time', '+weak caliper=mpi,time']
        TEST_ANALYZE: ['true']
      # Test affinity and hwloc with caliper on
      - HOST: dane
        ARCHCONFIG: llnl-cluster
        BENCHMARK: [kripke]
        VARIANT: ['caliper=mpi,time hwloc=on affinity=on']
      # IOR needs a mount_point
      - HOST: dane
        ARCHCONFIG: llnl-cluster
        SYSTEM_ARGS: [mount_point="/p/lustre1"]
        BENCHMARK: ior
      # Test openmpi
      - HOST: dane
        ARCHCONFIG: llnl-cluster
        SYSTEM_ARGS: [mpi=openmpi]
        BENCHMARK: osu-micro-benchmarks
      # Compilers
      - HOST: dane
        ARCHCONFIG: llnl-cluster
        SYSTEM_ARGS: [compiler=gcc, compiler=clang]
        BENCHMARK: osu-micro-benchmarks
run_tests_slurm_matrix:
  extends: .run_tests_slurm
  tags: [shell, matrix]
  needs: [allocate_resources_matrix]
  parallel:
    matrix:
      - HOST: matrix
        ARCHCONFIG: llnl-matrix
        BENCHMARK:
          - amg2023
          - babelstream
          - kripke
          - laghos
          - lammps
          - raja-perf
          - saxpy
        VARIANT: [+cuda]
      - HOST: matrix
        ARCHCONFIG: llnl-matrix
        BENCHMARK: [sparta-snl]
        VARIANT: [+cuda~gpu-aware-mpi]
