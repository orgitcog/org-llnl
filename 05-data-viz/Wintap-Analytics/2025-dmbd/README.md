# Dynamic Malware Behaviorial Dataset
July 2025

## Introduction
1. This dataset has been made available to encourage cybersecurity AI researchers to build machine learning models to identify malware based on the dynamic behavior of software execution.

2. Both training data and test data are provided with labels.

## Data Generation
3. The dataset was generated by Lawrence Livermore National Laboratories. A total of 65,416 different samples of malware/cleanware were obtained from various sources. Each sample was executed in a cyber range and host logs were collected for 5 minutes.

4. Host logs were generated using the Wintap tool (O/S software developed by LLNL) [1]. Labels were assigned to each sample (malicious or benign) by checking the results from ~80 different anti-virus engines. A voting scheme was used to combine the results from the different AV engines into a single label (malicious, benign or unknown). Samples with no clear consensus from the AV engines (i.e. a label of unknown) were excluded from the dataset.

## Data Description
5. Wintap generates a comprehensive set of logs. The full data dictionary can be found here [2]. The following tables were selected as being of most relevance:
   * Process creation
   * Image Load
   * Network Connection
   * File Creation
   * Registry Events

6. As can be seen from [2] each Wintap Table contains many fields. Extensive analysis of the full dataset has shown that only a small subset of the fields are required in order to build a machine learning model to distinguish between malware and cleanware. To reduce data volumes further we only include events that occur as a consequence of the detonation (i.e. all normal Windows background activity has been removed). In other words the data represents the process tree of the detonated sample.

7. The data can be represented in a single event table, a sample of which is shown below.

![Figure 1: Sample of Event Data]

8. The fields are as follows:
   * experiment: unique identifier for the experiment
   * timestamp: time of event
   * RuleName: type of event
   * parent_name: name of parent process
   * child_name: name of child
   * parent_guid: unique identifier for parent process
   * child_guid: unique identifier for child process

9. Each row in the table represents a log of an event in 'parent', 'child' format. The parent is always a process. The following event types are present:
   * process_create: creation of a process
   * process_term: termination of a Process
   * detonation: a special type of process creation event - it is the detonation event for the malware/cleanware sample
   * image_load: loading of an image (usually a DLL) by a process
   * file_create: a process creating or modifying a file
   * net_connect: network connection by a process
   * reg_create_key: creation of registry key
   * reg_delete_key: deletion of registry key
   * reg_delete_value: deletion of registry value

10. The fields are as follows:
    * experiment: unique identifier for the experiment
    * timestamp: time of event
    * RuleName: type of event
    * parent_name: name of parent process
    * child_name: name of child
    * parent_guid: unique identifier for parent process
    * child_guid: unique identifier for child process

11. In the above table you can see that detonated process P139075.exe has loaded a number of images (e.g. Pa5788\I132549.dll) before it terminates.

12. All process names start with a 'P' followed by a unique number. Image names have a prefix of 'I', file names have a prefix of 'F', network names have a prefix of 'N' and registry names have a prefix of 'R'. In cases where full paths are specified, the path is replaced with 'Pa' followed by a unique number.

13. The dataset represents the detonation of 65,416 software samples. There are ~39 million rows of event data.

14. We also provide a label dataset, a sample of which is shown below.

![Figure 2: Sample of Label Data]

15. The experiment field can be used to join the labels with the event data.

16. Each label can be one of two values:
    * benign
    * malicious

17. The year represents the approximate date that the software sample was first seen.

## Baseline Machine Learning Results
18. Accompanying the data set is a baseline calculation that other cyber-AI researchers are encouraged to beat.

19. The results are summarized below.

| 10 Fold X-Validation | | Test Set | |
|---|---|---|---|
| Accuracy | ROC AUC | Accuracy | ROC AUC |
| 0.950 | 0.958 | 0.954 | 0.971 |

Table 1: Baseline Machine Learning Results

_Note: ROC AUC is for a FPR (False Positive Rate) of 0.1_

20. While the baseline results are fairly good, we find that results deteriorate when we test on samples seen in a later year compared to data in the training set.

21. There are no 2024 malicious samples in the training set (dates range from 2017 - 2023), but there are in the test set. We are particularly interested in models which perform well on data samples for years later than those present in training.

## References
[1] Wintap https://github.com/LLNL/Wintap
[2] Wintap Data and Dictionary https://gdo-wintap.llnl.gov

# Release
LLNL-CODE-837816