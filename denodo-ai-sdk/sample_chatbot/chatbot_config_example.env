## ==============================
##      Chatbot Configuration
##
##   This file contains the necessary parameters for configuring the chatbot.
##   Please review all sections carefully to ensure proper functionality.
##
##   Sections:
##   1- CHATBOT CONFIGURATION
##   2- LLM CONFIGURATION
##   3- VECTOR STORE CONFIGURATION
##   4- AI SDK CONFIGURATION
##   5- LANGFUSE
## ==============================

## ==============================
## 1. CHATBOT CONFIGURATION
## Set the host, port, and other general chatbot settings.
## ==============================

CHATBOT_HOST = 0.0.0.0
CHATBOT_PORT = 9992

# CHATBOT_ROOT_PATH allows the sample chatbot to run on a different path. For example, 
# if using CHATBOT_ROOT_PATH = denodo-ai-sdk-sample-chatbot, the sample chatbot UI will
# be accessible at localhost:9992/denodo-ai-sdk-sample-chatbot/
#CHATBOT_ROOT_PATH = "denodo-ai-sdk-sample-chatbot"

# If you want to activate HTTPS, write down the key and certificate path here.
#CHATBOT_SSL_CERT = cert.pem 
#CHATBOT_SSL_KEY = key.pem

# When deploying to production, you might want to use multiple workers (cores) by setting CHATBOT_WORKERS
CHATBOT_WORKERS = 1

# CHATBOT_TIMEOUT sets the worker timeout in seconds for production mode (Gunicorn/Uvicorn)
# This is especially important when using the analyst tool which can take 5-15 minutes
# Default is 1200 seconds (20 minutes)
#CHATBOT_TIMEOUT = 1200

# You can set CHATBOT_DEEPQUERY to 0 to disable the DeepQuery feature.
CHATBOT_DEEPQUERY = 1

# If CHATBOT_REPORTING is set to 1, all the users queries, responses and token usage will be recorded in reports/user_report.csv
# Please set it to 0 to avoid this behaviour.
CHATBOT_REPORTING = 1

# Max side in megabytes of the reports generated in reports/. Will create a new timestamped one after reaching maxsize.
# Defaults to 10 mb.
CHATBOT_REPORT_MAX_SIZE=10

# Maximum number of report CSV files to keep in the reports/ directory.
# Set to 0 to disable the limit. Defaults to 10.
CHATBOT_REPORT_MAX_FILES=10

# By default, users can add feedback (positive or negative) + details on specific interactions.
# This feedback is added to the report in reports/user_report.csv
CHATBOT_FEEDBACK = 1

# By default, users can upload their own unstructured data through CSV files and ask questions about their documents.
# Set CHATBOT_UNSTRUCTURED_MODE = 0 to disable it.
CHATBOT_UNSTRUCTURED_MODE=1

# Allow users to edit LLM settings (provider, model, temperature, max_tokens) through the settings modal.
# Set to 1 to enable LLM editing, 0 to disable. This setting cannot be changed after startup.
CHATBOT_USER_EDIT_LLM = 0

# Instead of having users upload their own unstructured data through a CSV file, you can connect the chatbot to an already populated
# vector store. Please set CHATBOT_UNSTRUCTURED_MODE to 0 to avoid users overwriting this with their own CSVs.
#CHATBOT_UNSTRUCTURED_INDEX=custom_kb
#CHATBOT_UNSTRUCTURED_DESCRIPTION="Use this knowledge base whenever you receive questions about the Denodo software"

# The chatbot LLM might decide to request a graph generation even when not explicitly asked for in your question.
# To inject the chatbot LLM to only generate a graph when explicitly requested, disable CHATBOT_AUTO_GRAPH=0.
CHATBOT_AUTO_GRAPH=1

# Frontend timeout (in ms) for the vector store synchronization call.
CHATBOT_SYNC_VDBS_TIMEOUT = 600000

# Whether to verify SSL certificates for AI SDK requests.
# Set to 1 to enable SSL verification; 0 to disable (default: 0).
AI_SDK_VERIFY_SSL = 0

# If working in an offline, restricted environment and you need to connect to external endpoints with custom certificates,
# please use the following variables to point to those certificates for SSL verification.
# NOTE: The following path has been provided as reference, please set your own.
#REQUESTS_CA_BUNDLE=/home/denodo9_dev/denodo-ai-sdk-public-9-dev/denodo-ai-sdk-public-9-dev/SampleCA.cer
#SSL_CERT_FILE=/home/denodo9_dev/denodo-ai-sdk-public-9-dev/denodo-ai-sdk-public-9-dev/SampleCA.cer

## ==============================
## 2. LLM/Embeddings CONFIGURATION
## Specify the LLM provider, model, and API key for both LLM and embeddings model
## ==============================

CHATBOT_LLM_PROVIDER = openai
CHATBOT_LLM_MODEL = gpt-4o

# CHATBOT_LLM_TEMPERATURE defines the temperature setting for the chatbot LLM (0.0 to 2.0 for OpenAI, 0.0 to 1.0 for others)
CHATBOT_LLM_TEMPERATURE = 0.0

# CHATBOT_LLM_MAX_TOKENS defines the maximum OUTPUT tokens for the chatbot LLM. Not recommended to decrease this value.
CHATBOT_LLM_MAX_TOKENS = 4096

# Maximum number of rows from the VQL execution result that are passed to the LLM for generating the final answer.
#CHATBOT_LLM_RESPONSE_ROWS_LIMIT=15

CHATBOT_EMBEDDINGS_PROVIDER = openai
CHATBOT_EMBEDDINGS_MODEL = text-embedding-3-large

##==============================
## Ollama
## There is no specific configuration for Ollama, but:
##      - You must have the model already installed first through Ollama
##      - You must use the same model ID in CHAT_MODEL and SQL_GENERATION_MODEL as the one you use in Ollama
## There's no need to execute 'ollama run <model-id>' to use it in the AI SDK.
## The SDK will automatically attempt to connect to the default Ollama base URL and port, but
## you can modify this using the parameter OLLAMA_API_BASE_URL.
##==============================

#OLLAMA_API_BASE_URL = http://localhost:11434

##==============================
## OpenAI
## If you want to have two different OpenAI-API compatible providers, please check the user manual.
##==============================

# OPENAI_API_KEY defines the API key for your OpenAI account.

OPENAI_API_KEY = 

# OpenAI base url can be set OPTIONALLY if using a OpenAI-compatible provider different from OpenAI.

#OPENAI_BASE_URL = 

# OpenAI proxy to use. Set as http://{user}:{pwd}@{host}:{port} format
#OPENAI_PROXY_URL = 

# Set to 1 to verify the SSL certificate of the proxy. Defaults to 0 (off).
#OPENAI_PROXY_VERIFY_SSL = 0

# OpenAI organization ID. If not set it will use the default.
#OPENAI_ORG_ID = 

# You can set the dimensions (size of the vector object) with this variable for the embeddings model.
# It is recommended to leave it to the model's default dimension size.

#OPENAI_EMBEDDINGS_DIMENSIONS = 

##==============================
## Azure
## Please set the deployment name in the CHAT_MODEL/SQL_GENERATION_MODEL variables.
## The model (deployment) name used will be appended to the Azure endpoint, like this:
## /deployments/{CHAT_MODEL}
##==============================

# AZURE_ENDPOINT and AZURE_API_VERSION define the connection string and version to your Azure instance.
# AZURE_ENDPOINT refers to the everything until the azure.com domain. For example: https://example-resource.openai.azure.com/
# The AI SDK will automatically append /openai/deployments/{model_name}/chat/completions... to the endpoint.

AZURE_ENDPOINT = 
AZURE_API_VERSION = 

#AZURE_API_KEY defines the API key for your Azure account.
AZURE_API_KEY = 

# Azure proxy to use. Set as http://{user}:{pwd}@{host}:{port} format
#AZURE_PROXY = 

# Set to 1 to verify the SSL certificate of the proxy. Defaults to 0 (off).
#AZURE_PROXY_VERIFY_SSL = 0

# You can set the dimensions (size of the vector object) with this variable for the embeddings model.
# It is recommended to leave it to the model's default dimension size.

#AZURE_EMBEDDINGS_DIMENSIONS = 

##==============================
## Google
## NOTE: This is Google Cloud's Vertex AI service. Meant for production.
## A JSON service account with permissions is needed as application credentials.
##==============================

# GOOGLE_APPLICATION_CREDENTIALS defines the path to the JSON storing your Google Application Credentials

GOOGLE_APPLICATION_CREDENTIALS = 

# Set the number of tokens allocated for Google's thinking process.
# This controls how much the model can "think" before generating the final response.
# Higher values allow for more thorough reasoning but consume more tokens.
#GOOGLE_THINKING_TOKENS = 2000 

##==============================
## Google AI Studio
## NOTE: Not intended for production
##==============================

# GOOGLE_AI_STUDIO_API_KEY is your Google AI Studio API key

GOOGLE_AI_STUDIO_API_KEY = 

##==============================
## Groq
##==============================

# GROQ_API_KEY defines the API key for your OpenAI account.

GROQ_API_KEY = 

##==============================
## OpenRouter
##==============================

# OPENROUTER_API_KEY defines the API key for your OpenRouter account.

OPENROUTER_API_KEY = 

# In a comma-separated list, you can specify the providers you prefer OpenRouter route your LLM calls to.
#OPENROUTER_PREFERRED_PROVIDERS = 

##==============================
## SambaNova
##==============================

# SAMBANOVA_API_KEY defines the API key for your SambaNova account.

SAMBANOVA_API_KEY = 

##==============================
## Bedrock
##==============================

# If your LLM_MODEL is an AWS ARN, specifying the model provider (e.g., 'anthropic') is MANDATORY.
# It can be specified directly in the LLM_MODEL variable using the 'provider:arn:...' format.

# AWS_REGION, AWS_PROFILE_NAME, AWS_ROLE_ARN, AWS_SECRET_ACCESS_KEY, and AWS_ACCESS_KEY_ID define the connection parameters to your AWS Bedrock instance.
# If using EC2 with an IAM profile, you only need to set AWS_REGION to select the region you want to deploy Bedrock in.
# This is relevant because different regions have different models/costs/latencies and it is a required parameter by AWS when making a call.

AWS_REGION = 
AWS_PROFILE_NAME = 
AWS_ROLE_ARN = 
AWS_ACCESS_KEY_ID = 
AWS_SECRET_ACCESS_KEY = 

# By default, AWS SDK use the global STS endpoint (https://sts.amazonaws.com).
# To enable region-specific STS endpoints (e.g., https://sts.us-west-2.amazonaws.com),
# uncomment the following line:
#AWS_STS_REGIONAL_ENDPOINTS=regional

# Set the number of tokens allocated for Claude's thinking process.
# This controls how much the model can "think" before generating the final response.
# Higher values allow for more thorough reasoning but consume more tokens.
#AWS_CLAUDE_THINKING_TOKENS = 2000

##==============================
## Mistral
##==============================

MISTRAL_API_KEY = 

##==============================
## NVIDIA NIM
##==============================

NVIDIA_API_KEY = 

# If self-hosting NVIDIA NIM, set the base url here, like "http://localhost:8000/v1"
#NVIDIA_BASE_URL = 

##==============================
## Custom Providers
## You can configure any non-standard provider or multiple instances of the same provider type (e.g., two different Azure endpoints).
##==============================

# ------------------------------
# Example 1: Custom OpenAI-Compatible Provider
# ------------------------------
# Use this for any LLM that exposes an OpenAI-compatible API.
# 1. Choose a provider name, for example: MY_API
# 2. Set it in section 4, e.g., LLM_PROVIDER = MY_API
# 3. Define its parameters, following the pattern: [PROVIDER_NAME]_API_KEY, [PROVIDER_NAME]_BASE_URL, etc.

#MY_API_API_KEY =
#MY_API_BASE_URL =
#MY_API_PROXY = 
#MY_API_PROXY_VERIFY_SSL = 0

# ------------------------------
# Example 2: Custom Azure Providers
# ------------------------------
# Use this when you need separate configurations for LLM and embeddings, especially if they use different API versions.
# 1. Choose provider names that MUST start with "AZURE_", for example: AZURE_LLM and AZURE_EMBEDDING
# 2. Set them in section 4:
#    LLM_PROVIDER = AZURE_LLM
#    EMBEDDINGS_PROVIDER = AZURE_EMBEDDING
# 3. Define their parameters, following the pattern: [PROVIDER_NAME]_ENDPOINT, [PROVIDER_NAME]_API_VERSION, etc.

# --- Configuration for the LLM provider ---
#AZURE_LLM_ENDPOINT =
#AZURE_LLM_API_KEY =
#AZURE_LLM_API_VERSION =
#AZURE_LLM_PROXY =
#AZURE_LLM_PROXY_VERIFY_SSL = 0

# --- Configuration for the EMBEDDING provider (with a different API version) ---
#AZURE_EMBEDDING_ENDPOINT =
#AZURE_EMBEDDING_API_KEY =
#AZURE_EMBEDDING_API_VERSION =
#AZURE_EMBEDDING_EMBEDDINGS_DIMENSIONS =

##==============================
## Custom Headers
## You can define custom HTTP headers to be sent with API requests.
## This is useful for passing authentication tokens, subscription keys, or other metadata.
##==============================

# IMPORTANT: This functionality is specifically designed for providers
# compatible with the OpenAI/Azure API structure. It applies to:
# - "OpenAI" (standard)
# - "Azure" (standard)
# - Custom OpenAI-compatible providers (e.g., "MYAPI")
# - Custom Azure-compatible providers (e.g., "AZURE_DEV")

# The syntax is: PROVIDER_NAME_HEADER_Header-Name=HeaderValue
# - Replace PROVIDER_NAME with the provider's name in uppercase (e.g., MYAPI, AZURE).
# - Replace Header-Name with the actual HTTP header name (e.g., Ocp-Apim-Subscription-Key).
# - Replace HeaderValue with the value for that header.

# Example for a Custom OpenAI-compatible provider named "MYAPI"
#MYAPI_HEADER_X-Subscription-Key=HeaderValue

## ==============================
## 3. VECTOR STORE CONFIGURATION
## Specify the vector store provider.
## Available: Chroma, PGVector, OpenSearch
## ==============================

CHATBOT_VECTOR_STORE_PROVIDER = chroma

##==============================
## PGVector
##==============================

# The full connection string to the PGVector with username and password. For example: postgresql+psycopg://langchain:langchain@localhost:6024/langchain
# Must include postgresql+psycopg at the beginning. After that it's user:pwd@host:port/db

PGVECTOR_CONNECTION_STRING = 

##==============================
## OpenSearch
##==============================

# The URL of the OpenSearch instance. Default: http://localhost:9200
OPENSEARCH_URL =
OPENSEARCH_USERNAME =
OPENSEARCH_PASSWORD = 

## ==============================
## 4. AI SDK CONFIGURATION
## Set the connection details for the AI SDK.
## The USERNAME and PASSWORD are needed if you want chatbot users to be able to sync the vector store from the chatbot.
## These credentials must be the credentials of the user with permissions to execute the getMetadata endpoint.
## ==============================

AI_SDK_URL = http://localhost:8008
AI_SDK_USERNAME = admin
AI_SDK_PASSWORD = admin

#Only the AI SDK server has access to the Data Catalog URL.
#If you want to expose it in the chatbot for automatic view linking, include the DC URL here
#CHATBOT_DATA_CATALOG_URL = 

## ==============================
## 5. LANGFUSE (telemetry/tracing)
## Configure Langfuse to collect traces from the AI SDK and observe in real-time the pipeline at work.
## ==============================

# LANGFUSE_PUBLIC_KEY and LANGFUSE_SECRET_KEY identify your Langfuse project.
# Both are required to enable tracing.
#LANGFUSE_PUBLIC_KEY = 
#LANGFUSE_SECRET_KEY = 

# Set the Langfuse host here. Can either use cloud or local (open-source)
#LANGFUSE_HOST = https://us.cloud.langfuse.com

# Optional: Associate traces with a user identifier (appears as user_id in Langfuse)
#LANGFUSE_USER = 