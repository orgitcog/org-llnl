{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e02340b-c180-4c10-a20c-b95b82cba056",
   "metadata": {},
   "source": [
    "# MPI Communication with Kosh Data Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954aba90-a01e-4768-ac60-8c483f37a1fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "This example shows ways to read in and operate on datasets that are too large to fit in memory. In this case we will use three processes, but the code would work with any number less than the data size. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e5e519-a47c-4fb7-bed2-a51ef1155a40",
   "metadata": {},
   "source": [
    "## Create a large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51b6b3e-fff3-4053-925f-3d5a17022d30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "\n",
    "# Initialize MPI for communication between processes\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "nprocs = comm.Get_size()\n",
    "\n",
    "# All ranks need this information\n",
    "h5_file = \"my_data.h5\"\n",
    "dataset_name = \"normal\"\n",
    "data = np.empty((0, 5))\n",
    "\n",
    "# Create a large dataset on rank 0\n",
    "if rank == 0:\n",
    "    # Create an empty array with 539786 rows and 5 columns.\n",
    "    data = np.zeros((53786, 5))\n",
    "\n",
    "    # Generate random normal values for each column, setting location (mean) to the column index.\n",
    "    # Column 0 will have close to a mean of zero, column 1 will have close to a mean of 1, etc.\n",
    "    for i in range(5):\n",
    "        data[:, i] = np.random.normal(loc=i, scale=1, size=53786)\n",
    "    print(data.shape);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b17dc7-f453-4d99-8273-ff37a7a08811",
   "metadata": {},
   "source": [
    "(53786, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8066a3b-b067-45d1-b9b5-ab2f5a9a3f6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "if rank == 0:\n",
    "    # Save to hdf5 file\n",
    "    with h5py.File(h5_file, \"w\") as f:\n",
    "        f.create_dataset(dataset_name, data=data)\n",
    "\n",
    "# If running in a script you want other ranks to wait for rank 0\n",
    "comm.Barrier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dd2ce3-f9e4-46dd-8a9c-0d334c93bf4c",
   "metadata": {},
   "source": [
    "We can store and organize all our datasets in a Kosh store and associate the hdf5 file to a dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc129da6-4a9e-44f2-b0ba-9750e3b32744",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kosh\n",
    "\n",
    "store_path = \"data_slicing.sql\"\n",
    "# Only rank 0 will create the store and dataset\n",
    "# and remove any previously created datasets\n",
    "if rank == 0:\n",
    "    store = kosh.connect(store_path, delete_all_contents=True)\n",
    "    dset = store.create(\"example_1\")\n",
    "    dset.associate(h5_file, 'hdf5')\n",
    "    print(f\"Rank {rank} dset {dset}\"\n",
    "\n",
    "# The other ranks need to wait for rank 0\n",
    "comm.Barrier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea66620-a6c8-4b66-89ce-9e23bc7adb62",
   "metadata": {},
   "source": [
    "Rank 0 dset KOSH DATASET <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   id: 9c4f3ef134314957af96eb3e428bcd57 <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   name: example_1 <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   creator: Me <br>\n",
    "        \n",
    "--- Attributes --- <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   creator: Me <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   name: example_1 <br>\n",
    "--- Associated Data (1)--- <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    mime_type: hdf5 <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /my/directory/my_data.h5 ( 9b31268705bb43b392a8fe641b5dc8fa ) <br>\n",
    "--- Ensembles (0)--- <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  [ ] <br>\n",
    "--- Ensemble Attributes --- <br>\n",
    "--- Alias Feature Dictionary --- <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af399f0-1ee3-4282-9f06-6470d988e895",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# After rank 0 is done creating the store and dataset\n",
    "# the other ranks will connect to the store and find the dataset.\n",
    "if rank != 0:\n",
    "    # Other ranks will just read from the store\n",
    "    store = kosh.connect(store_path, read_only=True)\n",
    "    generator_obj = store.find(name=\"example_1\")\n",
    "    dset = next(generator_obj)\n",
    "\n",
    "# Show one of the other ranks has access to the same dataset\n",
    "if rank == 1:\n",
    "    print(f\"Rank {rank} dset {dset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dfef02-af39-4027-b6d0-fc94e6493db1",
   "metadata": {},
   "source": [
    "Rank 1 dset KOSH DATASET <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   id: 9c4f3ef134314957af96eb3e428bcd57 <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   name: example_1 <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   creator: Me <br>\n",
    "        \n",
    "--- Attributes --- <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   creator: Me <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   name: example_1 <br>\n",
    "--- Associated Data (1)--- <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    mime_type: hdf5 <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /my/directory/my_data.h5 ( 9b31268705bb43b392a8fe641b5dc8fa ) <br>\n",
    "--- Ensembles (0)--- <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  [ ] <br>\n",
    "--- Ensemble Attributes --- <br>\n",
    "--- Alias Feature Dictionary --- <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0669ccc7-8d1e-4fb9-98aa-ce9d74ad195a",
   "metadata": {},
   "source": [
    "We can see that the dataset created by rank 0 is the same one we have access to on rank 1. They have the same dataset ID, and associated data file ID.\n",
    "\n",
    "HDF5 already allows us to load slices of the data without reading in the entire dataset. \n",
    "Kosh's Default HDF5 Loader allows us to do the same thing with Kosh datasets pointing to HDF5 files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce38179-a6b0-47e7-b230-e83411cf2fbb",
   "metadata": {},
   "source": [
    "## Using MPI communication 3 ways with Kosh tools\n",
    "### 1. MPI Function with the default Kosh HDF5 loader\n",
    "We need to distribute the data across ranks or processors. Each process will run this function and since it is based on rank number, each one will read in a different chunk of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd51f50e-f68e-42f1-a570-dac3eb2dd369",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_slice(rank, nprocs, total_size):\n",
    "\n",
    "    # Calculate the chunk size for each rank\n",
    "    chunk_size = total_size // nprocs\n",
    "    remainder = total_size % nprocs\n",
    "\n",
    "    # Calculate start and end indices for each rank\n",
    "    start_index = rank * chunk_size + min(rank, remainder)\n",
    "    end_index = start_index + chunk_size + (1 if rank < remainder else 0)\n",
    "\n",
    "    return [start_index, end_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217ed7f3-f666-4a66-a419-ff39291e96ef",
   "metadata": {},
   "source": [
    "Next we want to use MPI communication to calculate statistics of each column of the dataset. We will calculate the minimum, maximum, and mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39397721-3288-4cb3-9f10-8ff03dfe4bb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_global_stats(local_data, total_size, comm):\n",
    "\n",
    "    # Now we can process the local data\n",
    "    local_min = np.min(local_data, axis=0)\n",
    "    local_max = np.max(local_data, axis=0)\n",
    "    local_sum = np.sum(local_data, axis=0)\n",
    "\n",
    "    # Use MPI comm to find global stats\n",
    "    global_min = local_min * 0.0\n",
    "    comm.Allreduce([local_min, MPI.DOUBLE],\n",
    "                   [global_min, MPI.DOUBLE],\n",
    "                   op=MPI.MIN)\n",
    "\n",
    "    global_max = local_max * 0.0\n",
    "    comm.Allreduce([local_max, MPI.DOUBLE],\n",
    "                   [global_max, MPI.DOUBLE],\n",
    "                   op=MPI.MAX)\n",
    "\n",
    "    global_sum = comm.allreduce(local_sum, op=MPI.SUM)\n",
    "    global_mean = global_sum / total_size\n",
    "\n",
    "    return [global_min, global_max, global_mean]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50956ec-0e1d-47f3-ab57-72f63b967506",
   "metadata": {},
   "source": [
    "Let's use these functions with our Kosh store and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1446109-7533-4f69-a1b6-192161355b2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rank 0 will get the total size of the dataset\n",
    "total_size = 0\n",
    "if rank == 0:\n",
    "    total_size = next(dset[dataset_name].describe_entries())[\"size\"][0]\n",
    "#communicate data size to other ranks\n",
    "total_size = comm.bcast(total_size, root=0)\n",
    "print(f\"Rank {rank} has total data size: {total_size}\");\n",
    "\n",
    "start_index, end_index = get_slice(rank, nprocs, total_size)\n",
    "\n",
    "# Read the local portion of the dataset\n",
    "local_data = dset[dataset_name][slice(start_index, end_index)]\n",
    "\n",
    "# Each process now has its own portion of the dataset\n",
    "print(f\"Rank {rank} read in data size: {local_data.shape}\");\n",
    "\n",
    "global_min, global_max, global_mean = get_global_stats(local_data,\n",
    "                                                       total_size,\n",
    "                                                       comm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dc54a1-b061-4c3e-a10b-a58292041931",
   "metadata": {},
   "source": [
    "Rank 0 has total data size: 53786 <br>\n",
    "Rank 0 read in data size: (17929, 5) <br>\n",
    "Rank 1 has total data size: 53786 <br>\n",
    "Rank 1 read in data size: (17929, 5) <br>\n",
    "Rank 2 has total data size: 53786 <br>\n",
    "Rank 2 read in data size: (17928, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe40f82b-8bdd-45e6-b7b7-5c7e966ed38d",
   "metadata": {},
   "source": [
    "Each process was able to communicate the local statsistics, and process 0 did the final communication to compute the global statistics for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181b691c-812e-4a06-a755-f2fb9d718274",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " if rank == 0:\n",
    "    print(\"Data stats: min, max, mean\\n\");\n",
    "for i in range(len(global_min)):\n",
    "    if rank == 0:\n",
    "        print(f\"Column {i}: {global_min[i]}, {global_max[i]}, {global_mean[i]}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a880ae18-83f1-42a4-877a-a26a315df120",
   "metadata": {},
   "source": [
    "Data stats: min, max, mean\n",
    "\n",
    "Column 0: -3.8022259459206973, 4.299911205123149, -0.0058177973219273324 <br>\n",
    "Column 1: -3.3927571082494996, 5.634167696696126, 0.9956082957507477 <br>\n",
    "Column 2: -2.2467429817991267, 6.252497478206577, 1.9941243339832948v <br>\n",
    "Column 3: -1.1730922554311096, 7.1547535770778525, 2.9961969445688132 <br>\n",
    "Column 4: 0.018511455979809632, 8.403950647261377, 4.005149971171246"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9c039d-cbd6-4adb-8b8c-7c4be7bcdbc8",
   "metadata": {},
   "source": [
    "### 2. MPI Function with a custom Kosh loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2834c3-022a-4632-913f-3f15f0df9478",
   "metadata": {},
   "source": [
    "Kosh offers loaders for these types of data files.\n",
    "\n",
    "    HDF5\n",
    "    \n",
    "    json\n",
    "    \n",
    "    numpy\n",
    "    \n",
    "    pandas\n",
    "    \n",
    "    pgm\n",
    "    \n",
    "    pil\n",
    "    \n",
    "    sidre\n",
    "\n",
    "Only the default HDF5 and numpy text loaders have the ability to enable distributed data loading. \n",
    "\n",
    "However, users may create custom loaders with \\_\\_getitem\\_\\_ so we can slice a large dataset and load different chunks on each processor. The numpy text loader below is a good example of how to use \\_\\_getitem\\_\\_ in a custom loader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a444d1-cdab-4221-9dbf-6c3b0fba35e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NumpyTxtLoader(kosh.KoshLoader):\n",
    "    types = {\"numpy/txt\": [\"numpy\", ]}\n",
    "\n",
    "    def _setup_via_metadata(self):\n",
    "        # use metadata to identify\n",
    "        self.skiprows = getattr(self.obj, \"skiprows\", 0)\n",
    "        self.features_at_line = getattr(self.obj, \"features_line\", None)\n",
    "        self.features_separator = getattr(self.obj, \"features_separator\", None)\n",
    "        self.columns_width = getattr(self.obj, \"columns_width\", None)\n",
    "\n",
    "    def list_features(self, *args, **kargs):\n",
    "        self._setup_via_metadata()\n",
    "        if self.features_at_line is None:\n",
    "            return [\"features\", ]\n",
    "        else:\n",
    "            with open(self.obj.uri) as f:\n",
    "                line = -1\n",
    "                while line < self.features_at_line:\n",
    "                    st = f.readline()\n",
    "                    line += 1\n",
    "                if self.columns_width is not None:\n",
    "                    features = [st[i:i + self.columns_width].strip()\n",
    "                                for i in range(0, len(st), self.columns_width)]\n",
    "                else:\n",
    "                    while st[0] == \"#\":\n",
    "                        st = st[1:]\n",
    "                    st = st.strip()\n",
    "                    features = st.split(self.features_separator)\n",
    "            return features\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        self._setup_via_metadata()\n",
    "        original_key = key\n",
    "        if isinstance(key, tuple):\n",
    "            # double subset\n",
    "            key, key2 = key[:2]  # ignore if more is sent\n",
    "        else:\n",
    "            key2 = None\n",
    "        if isinstance(key, int):\n",
    "            key = slice(key, key + 1)\n",
    "\n",
    "        if isinstance(key, slice):\n",
    "            start = key.start\n",
    "            stop = key.stop\n",
    "            step = key.step\n",
    "            if step is None:\n",
    "                step = 1\n",
    "            if (start is not None and start < 0) or \\\n",
    "                    (stop is not None and stop < 0):\n",
    "                # Doh we need to count lines\n",
    "                nlines = number_of_lines(self.obj.uri)\n",
    "                if start is not None and start < 0:\n",
    "                    start = nlines + start\n",
    "                if stop is not None and stop < 0:\n",
    "                    stop = nlines + stop\n",
    "            # ok if it's neg step we need to flip these two\n",
    "            # it has to do with numpy loader starting at a row for n row\n",
    "            # not reading a range\n",
    "            if step < 0:\n",
    "                start, stop = stop, start\n",
    "                if stop is not None:\n",
    "                    stop += 1  # because slice if exclusive on the end\n",
    "            if start is None:\n",
    "                start = self.skiprows\n",
    "            else:\n",
    "                start += self.skiprows\n",
    "            if stop is not None:\n",
    "                max_rows = stop - start + self.skiprows\n",
    "            else:\n",
    "                max_rows = None\n",
    "\n",
    "            if max_rows is None or max_rows > 0:\n",
    "                # , usecols=numpy.arange(key2.start, key2.stop, key2.step))\n",
    "                data = numpy.loadtxt(\n",
    "                    self.obj.uri, skiprows=start, max_rows=max_rows)\n",
    "            else:\n",
    "                # , usecols=numpy.arange(key2.start, key2.stop, key2.step))\n",
    "                data = numpy.loadtxt(\n",
    "                    self.obj.uri,\n",
    "                    skiprows=start,\n",
    "                    max_rows=2)[\n",
    "                    0:2:-1]\n",
    "            if data.ndim > 1:\n",
    "                if key2 is not None:\n",
    "                    data = data[::step, key2]\n",
    "                elif step != 1:  # useless if step is 1\n",
    "                    data = data[::step]\n",
    "            else:\n",
    "                if key.step is not None and key.step != 1:\n",
    "                    data = data[::step]\n",
    "                else:\n",
    "                    if key2 is not None:\n",
    "                        data = data[key2]\n",
    "                    else:\n",
    "                        data = data\n",
    "        else:\n",
    "            raise KeyError(\"Invalid key value: {}\".format(original_key))\n",
    "        if self.features_at_line is None:\n",
    "            return data\n",
    "        else:\n",
    "            feature_index = self.list_features().index(self.feature)\n",
    "            return data[:, feature_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26243a16-ffd7-4775-9e80-8476d23436c1",
   "metadata": {},
   "source": [
    "Let's try using this loader with the getitem functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6937cd1e-5173-4f54-b5ba-a11f08673f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create txt file using the same dataset as in the previous example\n",
    "txt_name = 'array.out'\n",
    "if rank == 0:\n",
    "    txt_data = np.savetxt(txt_name, data)\n",
    "\n",
    "# We will use the same store as before, and add another Kosh dataset.\n",
    "# We can save the data size information for later use.\n",
    "data_shape = comm.bcast(data.shape, root=0)\n",
    "# Only rank 0 will create the dataset\n",
    "if rank == 0:\n",
    "    metadata = {'size': data_shape[0]}\n",
    "    dset2  = store.create(\"array\", metadata=metadata, features_separator=',')\n",
    "    # Associate the files to the Kosh dataset\n",
    "    dset2.associate(txt_name, mime_type='numpy/txt')\n",
    "\n",
    "    # Get the total size of the dataset\n",
    "    total_size = getattr(dset2, \"size\")\n",
    "    print(f\"Total data size: {total_size}\")\n",
    "\n",
    "# Other ranks will wait for 0 to finish work\n",
    "comm.Barrier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48624c4-c861-4b1a-8ea4-1f673503a19e",
   "metadata": {},
   "source": [
    "Total data size: 53786"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb252910-744b-405c-927b-4d8cd932267a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The other ranks will search for the dataset based on the name\n",
    "if rank != 0:\n",
    "    generator_obj = store.find(name=\"array\")\n",
    "    dset2 = next(generator_obj)\n",
    "    # Show rank 2 has dset2\n",
    "    if rank == 2:\n",
    "        print(f\"Rank {rank} dset {dset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc0e8a5-d4cf-4c90-bfca-7fb021519f6d",
   "metadata": {},
   "source": [
    "Rank 2 dset KOSH DATASET <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   id: 34da205abf4f4439b62e60528e384efc <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   name: array <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   creator: Me <br>\n",
    "        \n",
    "--- Attributes --- <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   creator: Me <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   name: array <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   size: 53786 <br>\n",
    "--- Associated Data (1)--- <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    mime_type: numpy/txt <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; /my/directory/array.out ( 34f9c5fe8bc74ac5b06370cb1fe1c515 ) <br>\n",
    "--- Ensembles (0)--- <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  [ ] <br>\n",
    "--- Ensemble Attributes --- <br>\n",
    "--- Alias Feature Dictionary --- <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6fd013-4bf4-41d1-9fa2-4120743af8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index, end_index = get_slice(rank, nprocs, total_size)\n",
    "\n",
    "# Read the local portion of the dataset\n",
    "local_data = dset2[\"features\"][slice(start_index, end_index)]\n",
    "\n",
    "# Each process now has its own portion of the dataset\n",
    "print(f\"Rank {rank} has data size: {local_data.shape}\");\n",
    "\n",
    "global_min, global_max, global_mean = get_global_stats(local_data,\n",
    "                                                       total_size,\n",
    "                                                       comm)\n",
    "\n",
    "# Each process was able to communicate the local statsistics, and process 0 did\n",
    "# the final communication to compute the global statistics for each column.\n",
    "\n",
    "print(\"Data stats: min, max, mean\\n\");\n",
    "for i in range(len(global_min)):\n",
    "    print(f\"Column {i}: {global_min[i]}, {global_max[i]}, {global_mean[i]}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbe9827-e343-4f01-9892-106fb5432233",
   "metadata": {},
   "source": [
    "Rank 0 has data size: (17929, 5) <br>\n",
    "Rank 1 has data size: (17929, 5) <br>\n",
    "Rank 2 has data size: (17928, 5) <br>\n",
    "Data stats: min, max, mean\n",
    "\n",
    "Column 0: -3.8022259459206973, 4.299911205123149, -0.0058177973219273324 <br>\n",
    "Column 1: -3.3927571082494996, 5.634167696696126, 0.9956082957507477 <br>\n",
    "Column 2: -2.2467429817991267, 6.252497478206577, 1.9941243339832948v <br>\n",
    "Column 3: -1.1730922554311096, 7.1547535770778525, 2.9961969445688132 <br>\n",
    "Column 4: 0.018511455979809632, 8.403950647261377, 4.005149971171246"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536b406d-5759-4eaf-9cbb-1cfe344f8138",
   "metadata": {},
   "source": [
    "We are able to distribute the data array across the processes with the getitem function in the numpy txt loader, and come to the same conclusion as in the hdf5 case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b243011-4b5c-4600-886b-c98f8431e209",
   "metadata": {},
   "source": [
    "### 3. Using a Kosh operator for MPI functions with a parallel enabled loader\n",
    "Let's continue our example with an HDF5 data file but this time the MPI functions will take place in a Kosh operator, and using the global min and max we can return normalized dataset chunks to each process.\n",
    "\n",
    "We will create a few arrays with varying row sizes, but all with 5 columns. We only create the data with rank 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91f6f7b-3ad0-4518-a4ed-7101d52f8268",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if rank == 0:\n",
    "    for n in range(3):\n",
    "        size = (n + 1) * 60\n",
    "        # Create an empty array with 5386 rows and 5 columns\n",
    "        data = np.zeros((size, 5))\n",
    "\n",
    "        # Generate random normal values for each column, setting location (mean) to the column index\n",
    "        # Column 0 will have close to a mean of zero, column 1 will have close to a mean of 1, etc.\n",
    "        for i in range(5):\n",
    "            data[:, i] = np.random.normal(loc=i, scale=1, size=size)\n",
    "\n",
    "        # Save to hdf5 file\n",
    "        h5_file = f\"my_data{n}.h5\"\n",
    "        dataset_name = \"normal\"\n",
    "        with h5py.File(h5_file, \"w\") as f:\n",
    "            f.create_dataset(dataset_name, data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeed3eb-c541-4bea-858b-d016e316e392",
   "metadata": {},
   "source": [
    "We will use the same store as before, and add another Kosh dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f73d31-fec6-4e48-8e05-1cb4a2fb46c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rank 0 will create the datasets and associate the files\n",
    "if rank == 0:\n",
    "    dset3  = store.create()\n",
    "    dset4 = store.create()\n",
    "    dset5 = store.create()\n",
    "\n",
    "    # Associate the files to the Kosh dataset\n",
    "    dset3.associate(\"my_data0.h5\", 'hdf5')\n",
    "    dset4.associate(\"my_data1.h5\", 'hdf5')\n",
    "    dset5.associate(\"my_data2.h5\", 'hdf5')\n",
    "    \n",
    "# The other ranks need to wait for rank 0\n",
    "comm.Barrier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c4e5ff-2f7d-49ed-b0cf-83ee10fce36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The other ranks need to find the datasets\n",
    "if rank != 0:\n",
    "    generator_obj1 = store.find(name=\"data0\")\n",
    "    dset3 = next(generator_obj1)\n",
    "\n",
    "    generator_obj2 = store.find(name=\"data1\")\n",
    "    dset4 = next(generator_obj2)\n",
    "\n",
    "    generator_obj3 = store.find(name=\"data2\")\n",
    "    dset5 = next(generator_obj3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c1f485-d0a6-4f13-965a-cb4e267aa6e8",
   "metadata": {},
   "source": [
    "We need a function to assign data to each processor from multiple files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95aa6fa-3cc2-46b4-beb7-57cec4b1328f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def distribute_data(sizes, nprocs, rank):\n",
    "    breakpoint()\n",
    "    # Calculate the start and end indices for each process\n",
    "    total_size = sum(sizes)\n",
    "    start_idx = rank * total_size // nprocs\n",
    "    end_idx = (rank + 1) * total_size // nprocs\n",
    "\n",
    "    # Create a list to hold the local data sizes and corresponding file indices\n",
    "    local_data_info = []\n",
    "    current_size = 0\n",
    "\n",
    "    for i, size in enumerate(sizes):\n",
    "        if current_size + size > start_idx and current_size < end_idx:\n",
    "            # Calculate the number of rows to read for this dataset\n",
    "            if current_size < start_idx:\n",
    "                # Calculate the starting row for this dataset\n",
    "                start_row = start_idx - current_size\n",
    "            else:\n",
    "                start_row = 0\n",
    "            \n",
    "            if current_size + size > end_idx:\n",
    "                # Calculate the number of rows to read\n",
    "                end_row = end_idx - current_size\n",
    "            else:\n",
    "                end_row = size\n",
    "            \n",
    "            local_data_info.append((i, start_row, end_row))  # Store the index and row range\n",
    "        current_size += size\n",
    "\n",
    "    return local_data_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59260ed-81a5-4de4-9ed2-cdc715909b1b",
   "metadata": {},
   "source": [
    "Now we can create a custom Kosh operator that can distribute data evenly across the processors, calculate the global min and max, and then return normalized data chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd32a9d-0477-422f-b9f1-c807fca5a24e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MPINormalize(kosh.KoshOperator):\n",
    "\n",
    "    types = {\"numpy\": [\"numpy\", ]}\n",
    "\n",
    "    def __init__(self, *args, **options):\n",
    "        super(MPINormalize, self).__init__(*args, **options)\n",
    "        self.options = options\n",
    "\n",
    "        # Initialize MPI\n",
    "        self.comm = MPI.COMM_WORLD\n",
    "        self.rank = comm.Get_rank()\n",
    "        self.nprocs = comm.Get_size()\n",
    "\n",
    "    def operate(self, *inputs, **kargs):\n",
    "\n",
    "        # Get the sizes of each kosh dataset\n",
    "        input_sizes = []\n",
    "        total_size = 0\n",
    "        if rank == 0:\n",
    "            desc = list(self.describe_entries())\n",
    "            for i in range(len(inputs)):\n",
    "                input_sizes.append(desc[i][\"size\"][0])\n",
    "            total_size = sum(input_sizes)\n",
    "        input_sizes = comm.bcast(input_sizes, root=0)\n",
    "        total_size = comm.bcast(total_size, root=0)\n",
    "\n",
    "        local_data_info = distribute_data(input_sizes, nprocs, rank)\n",
    "\n",
    "        # Each process can now read its assigned dataset(s)\n",
    "        local_data = np.empty((0, 5), dtype=float)\n",
    "        for index, start, stop in local_data_info:\n",
    "            data = inputs[index][slice(start, stop)]\n",
    "            local_data = np.concatenate((local_data, data), axis=0)\n",
    "\n",
    "        # With MPI we calculate statistics for each column in the dataset\n",
    "        global_min, global_max, _ = get_global_stats(local_data,\n",
    "                                                     total_size,\n",
    "                                                     comm)\n",
    "\n",
    "        # Using the min and max we normalize each column of data\n",
    "        for f in range(len(global_min)):\n",
    "            local_data[:, f] = (local_data[:, f] - global_min[f]) / \\\n",
    "                (global_max[f] - global_min[f])\n",
    "\n",
    "        return local_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f545df6-3941-4fc0-b687-f4a2757e82c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaled_local = MPINormalize(dset3['normal'], dset4['normal'], dset5['normal'])[:]\n",
    "\n",
    "print(f\"Rank {rank} local data: \\n shape {scaled_local.shape} \\n min {scaled_local.min(axis=0)} \\n \\\n",
    "        max {scaled_local.max(axis=0)} \\n mean {scaled_local.mean(axis=0)}\");\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021fb155-2ef0-456e-8163-73a63c16bfbe",
   "metadata": {
    "tags": []
   },
   "source": [
    "Rank 0 local data: <br> \n",
    "shape (120, 5) <br> \n",
    "min  [0.        , 0.21420619, 0.15787334, 0.        , 0.        ] <br> \n",
    "max  [1.        , 0.87725257, 0.93489143, 0.99439837, 0.80387367] <br> \n",
    "mean [0.49013495, 0.58344045, 0.47229402, 0.58820366, 0.42868854] <br> \n",
    "\n",
    "Rank 1 local data: <br>\n",
    "shape (120, 5) <br>\n",
    "min  [0.08429409 0.18198548 0.         0.25029003 0.05370255] <br> \n",
    "max  [0.88083102 0.94954022 1.         1.         0.85265535] <br> \n",
    "mean [0.49276804 0.57804458 0.52172094 0.60238864 0.43822516] <br> \n",
    "\n",
    "Rank 2 local data: <br>\n",
    "shape (120, 5) <br>\n",
    "min  [0.03761877 0.         0.07890669 0.17243195 0.11313334] <br> \n",
    "max  [0.97382317 1.         0.99053639 0.99903149 1.        ] <br> \n",
    "mean [0.50205695 0.58495661 0.51220394 0.56918653 0.45209219] <br> \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127899a0-0003-4511-a504-c9b6abe6465e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_tik_env",
   "language": "python",
   "name": "weave_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
