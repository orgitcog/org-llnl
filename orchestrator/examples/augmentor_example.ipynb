{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d28542-d2dd-4125-8f19-84994fae2642",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from orchestrator.utils.setup_input import init_and_validate_module_type, setup_orch_modules, read_input\n",
    "from orchestrator.utils.input_output import ase_glob_read\n",
    "from orchestrator.utils.data_standard import SELECTION_MASK_KEY, SELECTOR_PROPERTY_MAP\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307b825f-0514-460d-8ec3-592e40ba0bad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set up our input dict\n",
    "all_inputs = {\n",
    "    \"storage\":{\n",
    "        \"storage_type\":\"COLABFIT\",\n",
    "        \"storage_args\":{\n",
    "            \"credential_file\":\"PATH/TO/your_credentials.json\"\n",
    "        }\n",
    "    },    \n",
    "    \"score\":{\n",
    "        \"score_type\":\"QUESTSEfficiencyScore\",\n",
    "        \"score_args\": {\n",
    "            \"bandwidth\": 0.02,\n",
    "            \"num_nearest_neighbors\": 3,\n",
    "            \"graphs_neighbors\": 10,\n",
    "            \"approx\": False,\n",
    "            \"descriptors_key\": \"quests_descriptor_descriptors\"\n",
    "        }\n",
    "    },\n",
    "    \"descriptor\": {\n",
    "        \"descriptor_type\": \"QUESTSDescriptor\",\n",
    "        \"descriptor_args\": {\n",
    "            \"num_nearest_neighbors\": 32,\n",
    "            \"cutoff\": 5.0\n",
    "        }\n",
    "    }, \n",
    "    \"augmentor\": {\n",
    "        \"augmentor_type\": \"BASE\", \n",
    "        \"augmentor_args\":{}\n",
    "        },    \n",
    "    \"trainer\": {\n",
    "        \"trainer_type\": \"FitSnap\", \n",
    "        \"trainer_args\": {}\n",
    "    },\n",
    "    \"potential\":{\n",
    "        \"potential_type\":\"FitSnap\",\n",
    "        \"potential_args\":{\n",
    "            \"settings_path\":\"fitsnap.in\",\n",
    "            \"model_driver\":\"no-driver\",\n",
    "            \"kim_item_type\": \"simulator-model\",\n",
    "            \"kim_api\":\"kim-api-collections-management\",\n",
    "            \"species\": [\"Ta\"]\n",
    "        }\n",
    "    },\n",
    "    \"workflow\": {\n",
    "        \"workflow_type\": \"LOCAL\", \n",
    "        \"workflow_args\":{\n",
    "            \"root_directory\":\"./workflow_output\"\n",
    "        }\n",
    "    }\n",
    "}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d08b4d-4df1-48b2-8866-d44fa9a1cf5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's make a FitSNAP input file\n",
    "fitsnap_input_string = f'''[BISPECTRUM]\n",
    "numTypes = 1\n",
    "twojmax = 6\n",
    "rcutfac = 4.0\n",
    "rfac0 = 0.99363\n",
    "rmin0 = 0.0\n",
    "wj = 1.0\n",
    "radelem = 0.5\n",
    "type = Ta\n",
    "wselfallflag = 0\n",
    "chemflag = 0\n",
    "bzeroflag = 1\n",
    "quadraticflag = 0\n",
    "\n",
    "[CALCULATOR]\n",
    "calculator = LAMMPSSNAP\n",
    "energy = 0\n",
    "force = 1\n",
    "stress = 0\n",
    "\n",
    "[SOLVER]\n",
    "solver = SVD\n",
    "compute_testerrs = 1\n",
    "detailed_errors = 1\n",
    "\n",
    "[OUTFILE]\n",
    "output_style = SNAP\n",
    "metrics = trained_potential_metrics.md\n",
    "potential = trained_potential\n",
    "\n",
    "[REFERENCE]\n",
    "units = metal\n",
    "atom_style = atomic\n",
    "pair_style = zero 10.0\n",
    "pair_coeff = * *\n",
    "\n",
    "[EXTRAS]\n",
    "dump_descriptors = 1\n",
    "dump_truth = 1\n",
    "dump_weights = 1\n",
    "\n",
    "[MEMORY]\n",
    "override = 0\n",
    "'''\n",
    "\n",
    "file = open(\"./fitsnap.in\",\"w\")\n",
    "file.write(fitsnap_input_string)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04201472-7c3e-466c-a40e-75134a616bc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build modules\n",
    "augmentor = init_and_validate_module_type('augmentor', all_inputs)\n",
    "descriptor = init_and_validate_module_type('descriptor', all_inputs)\n",
    "potential = init_and_validate_module_type('potential', all_inputs)\n",
    "score = init_and_validate_module_type('score', all_inputs)\n",
    "storage = init_and_validate_module_type('storage', all_inputs)\n",
    "trainer = init_and_validate_module_type('trainer', all_inputs)\n",
    "workflow = init_and_validate_module_type('workflow', all_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1e7525-24d5-497b-a187-c228d1771240",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# alternatively, use the setup_orch_modules to do the same in one line!\n",
    "augmentor, descriptor, _, potential, score, _, storage, _, trainer, workflow = setup_orch_modules(all_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed5b120-ffe9-4b9d-8d70-b0bbed78717c",
   "metadata": {},
   "source": [
    "## Part 0: Make a dataset to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702edd6c-8dae-45cd-835c-f42ffbb49d13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# as a simple example, we will use the small Ta dataset in the test folder - set the path accordingly\n",
    "path_to_Ta_dataset = 'PATH/TO/ORCH_ENV/orchestrator/orchestrator/test/shared_inputs/Ta_training_configs'\n",
    "# read in the files to add to storage\n",
    "configs = ase_glob_read(path_to_Ta_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73448601-f0c5-40a0-90d7-8739927ac539",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# when ingesting data into Storage, it is necessary to attach calculation metadata to ensure consistency \n",
    "# this is a simple example, though the Orchestrator will handle this for data that it creates\n",
    "parameters = {\n",
    "    # Fill both values with the relevant input parameters from the simulation.\n",
    "    # This example is for Quantum Espresso.\n",
    "    'code': {\n",
    "        'SYSTEM': {\n",
    "            'ecutwfc': 60  # Ry\n",
    "        }\n",
    "    },\n",
    "    # The DFT oracle used for the simulations should have a\n",
    "    # translate_universal_parameters() function that can be called and passed\n",
    "    # the values from the `code` section.\n",
    "    'universal': {\n",
    "        'code': 'Quantum Espresso',\n",
    "        'version': 'v7.4.1',\n",
    "        'planewave_cutoff': 816  # eV\n",
    "    }\n",
    "}\n",
    "\n",
    "# prepare the dataset metadata dictionary\n",
    "metadata = {\n",
    "    'description': 'bare bones example dataset for augmentor notebook',\n",
    "    'authors': 'Orchestrator example user',\n",
    "    'parameters': parameters\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1894e18-e71a-442e-8c82-540726d623d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we need to tell storage what properties we are interested in storing - in this case the defaults (energy, forces, stress) will work great for us!\n",
    "storage.set_default_property_map()\n",
    "# add the configurations to storage in a dataset named 'augmentor_example_dataset'\n",
    "initial_dataset_handle = storage.new_dataset('augmentor_example_dataset', configs, metadata)\n",
    "print(f'Added data as handle {initial_dataset_handle}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770c9b72-b41b-4e94-a488-1eb7946608b8",
   "metadata": {},
   "source": [
    "## Part 1: Prune the dataset \n",
    "\n",
    "Now we're set up to try the example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a73c1a-1b18-4473-8e5d-3f8c34225da6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First, pull the dataset from storage\n",
    "initial_dataset = storage.get_data(initial_dataset_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e02b63-f06c-4a69-a38a-da1330992586",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# next, we need to add descriptors to the data in order to help us prune based on structural similarity\n",
    "calc_ids = descriptor.run(\n",
    "    path_type='dataset_descriptors',\n",
    "    compute_args={},\n",
    "    configs=initial_dataset,\n",
    "    workflow=workflow,\n",
    "    batch_size=50,\n",
    ")\n",
    "# if using an asynchronous workflow (like slurm or lsf) calc_ids will return as soon as a job is \n",
    "# submitted to the scheduler so data_from_calc_ids may wait a while as the job completes\n",
    "configs_with_desc = descriptor.data_from_calc_ids(calc_ids, workflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bc0f20-68f2-4886-a232-fa506c2622c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# using colabfit, we can update the version of our dataset to incorporate the new information\n",
    "desc_handle = storage.update_data(\n",
    "    dataset_handle=initial_dataset_handle, \n",
    "    data=configs_with_desc,\n",
    "    # since we're adding new properties to store, we need to tell storage what they are\n",
    "    # Orchestrator modules that have data which can be stored will define the \n",
    "    # OUTPUT_KEY and property_map that should be used to denote their properties in storage\n",
    "    new_properties={descriptor.OUTPUT_KEY: descriptor.get_colabfit_property_map()},\n",
    "    # we can update the description to keep track of what has changed in the database\n",
    "    updated_description='example dataset with descriptors'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e190b8-2e6d-4b78-b3cf-c31ad6e531f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we can list all datasets in the database, or search by the name we gave it\n",
    "# note the updated description!\n",
    "storage.list_data('augmentor_example_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e7dcb2-aec2-4e85-b404-be0455ac0671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we'll prune the dataset\n",
    "pruned_configs = augmentor.iterative_fps_prune(\n",
    "#pruned_configs = augmentor.chunked_iterative_fps_prune(\n",
    "    dataset=configs_with_desc, \n",
    "    #dataset=subset,\n",
    "    descriptors_key=f'{descriptor.OUTPUT_KEY}_descriptors',\n",
    "    # we'll use the QUESTS Efficiency metric to direct our pruning\n",
    "    prune_approach=score,\n",
    "    num_chunks=1,\n",
    "    # we will be satisfied with a 50% efficient dataset for this example\n",
    "    # in practive you can likely go much higher (i.e. 0.01 = 99% efficient)\n",
    "    # since we're using an artificially and small and highly similar dataset to \n",
    "    # demonstrate, we'd end up pruning nearly the whole set if using typical parameters\n",
    "    pruning_convergence=0.5,\n",
    "    iteration_limit=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd25852-b483-48cc-a107-d20ea2caa140",
   "metadata": {},
   "source": [
    "Check out the log file to see the iteration step information from the pruning process!\n",
    "\n",
    "If you want to prune a fixed amount, check out the `simple_prune_dataset()` function with `prune_method = 'percentage'`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f09a45-909d-4849-85cd-e0f3356c6912",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_atoms = np.sum([len(x) for x in pruned_configs])\n",
    "atoms_after_pruning = np.sum(np.concatenate([x.get_array(SELECTION_MASK_KEY) for x in pruned_configs]))\n",
    "print(f'After pruning, a dataset with {total_atoms} atoms was reduced in size to {atoms_after_pruning} atoms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2678c3b5-a5f6-4bf9-b656-e227459c3cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the pruning information to the dataset in storage\n",
    "prune_handle = storage.update_data(\n",
    "    dataset_handle=desc_handle, \n",
    "    data=pruned_configs,\n",
    "    # since we're adding new properties to store, we need to tell storage what they are\n",
    "    # for the selection property mask, there are data standard constants that can be used to define the map\n",
    "    new_properties={SELECTOR_PROPERTY_MAP['new_property_name']: SELECTOR_PROPERTY_MAP['new_map']},\n",
    "    # we can update the description to keep track of what has changed in the database\n",
    "    updated_description='example pruned dataset with descriptors'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85f178d-69e7-499f-912c-d6566ab5d8d1",
   "metadata": {},
   "source": [
    "## Part 2: Train SNAP potentials using full and pruned datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d28614-b75d-4603-9208-b82a6e1dd087",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights = np.concatenate([c.get_array(SELECTION_MASK_KEY) for c in storage.get_data(prune_handle)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148db895-3b77-412d-ad64-055d6212512c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_id, per_atom_weights in zip([initial_dataset_handle, prune_handle], [False, True]):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    configs = storage.get_data(dataset_id)\n",
    "\n",
    "    # assemble the weight matrices to pass in for training\n",
    "    # (this is just needed if you are doing the pruned dataset)\n",
    "    if per_atom_weights:\n",
    "        weights = np.concatenate([c.get_array(SELECTION_MASK_KEY) for c in configs])\n",
    "        model_type = 'pruned'\n",
    "    else:\n",
    "        weights = False\n",
    "        model_type = 'full'\n",
    "        \n",
    "    model, loss = trainer.train(\n",
    "        'augmentor_example_training',\n",
    "        potential,\n",
    "        storage,\n",
    "        dataset_id,\n",
    "        workflow,\n",
    "        eweight=0,\n",
    "        fweight=1, # we'll do force only training since we are masking based on atomic environments\n",
    "        vweight=0,\n",
    "        per_atom_weights=weights,\n",
    "        # by default Orchestrator will record potentials using the kimkit repo but we will skip for this example\n",
    "        upload_to_kimkit=False \n",
    "    )\n",
    "    \n",
    "    # for SNAP potentials, the location and prefix of the files that define the IAP is saved as the .parameter_path attribute\n",
    "    model_path = potential.parameter_path\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    print(f'trained model for the {model_type} dataset can be found in {model_path}')\n",
    "    print(f'loading data and training SNAP potential for the {model_type} dataset took {elapsed_time} sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f4146b-17d0-491a-a5ca-f2dca4a200f0",
   "metadata": {},
   "source": [
    "The differences will be more noteable in real-world examples with datasets more than a few hundred atoms.\n",
    "\n",
    "Look in the output directories to see the potentials and their metric files (which can also be accessed as the loss object)\n",
    "\n",
    "For an example of how to use a Potential you train in simulations with the Orchestrator, checkout the LAMMPS_SNAP_example notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a43735-6240-4a73-bf59-744035f2f259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the test datasets we created\n",
    "for handle in [prune_handle, desc_handle, initial_dataset_handle]:\n",
    "    storage.delete_dataset(handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Orchestrator kernel (Aug)",
   "language": "python",
   "name": "aug_orchestrator_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
