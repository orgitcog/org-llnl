# OGhidra Configuration File
# This file contains environment variables for configuring the Ollama-GhidraMCP Bridge

# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434/
OLLAMA_MODEL=gemma3:27b # or devstral-2:123b-cloud if you wanna try something with a lil more umf
OLLAMA_EMBEDDING_MODEL=nomic-embed-text
OLLAMA_TIMEOUT=300

# LLM Logging Configuration
# Enable comprehensive logging of all LLM interactions (prompts, responses, timing, tokens)
LLM_LOGGING_ENABLED=true
LLM_LOG_FILE=logs/llm_interactions.log
LLM_LOG_PROMPTS=true
LLM_LOG_RESPONSES=true
LLM_LOG_TOKENS=true
LLM_LOG_TIMING=true
LLM_LOG_FORMAT=json

# Options for LLM_LOG_FORMAT:
# - json: Structured JSON format (recommended for parsing and analysis)
# - text: Human-readable text format

# To disable specific logging components while keeping LLM_LOGGING_ENABLED=true:
# - LLM_LOG_PROMPTS=false (don't log prompts)
# - LLM_LOG_RESPONSES=false (don't log responses)
# - LLM_LOG_TOKENS=false (don't log token counts)
# - LLM_LOG_TIMING=false (don't log timing information)

# GhidraMCP Configuration
GHIDRA_BASE_URL=http://localhost:8080/
GHIDRA_TIMEOUT=30
GHIDRA_MOCK_MODE=false

MAX_EXECUTION_STEPS=10    # Increase from 10 to 20
# ===== AGENTIC CYCLES (Outer Loop) ===== 
MAX_AGENTIC_CYCLES=2          # Investigation cycles
AGENTIC_LOOP_ENABLED=true         # Investigation cycles
AGENTIC_LOOP_ENABLED=true     # Enable adaptive re-planning


# Delay between ollama calls (some servers auto reject too many requests especially cloud hosting)
OLLAMA_REQUEST_DELAY=3.0 # Delay of 3.0 pervents OLLaMA from blocking you when using cloud models

# Maximum context tokens for prompts (4000-200000)
CONTEXT_BUDGET=100000

# Fraction of context budget for execution results (0.1-0.8)
CONTEXT_BUDGET_EXECUTION=0.4

# Enable LLM-based summarization for large results
ENABLE_RESULT_SUMMARIZATION=true

# Enable result caching with references
RESULT_CACHE_ENABLED=true

# Enable tiered context (detailed recent, summarized older)
TIERED_CONTEXT_ENABLED=true
